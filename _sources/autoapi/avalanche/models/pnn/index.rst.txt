:mod:`avalanche.models.pnn`
===========================

.. py:module:: avalanche.models.pnn


Module Contents
---------------

Classes
~~~~~~~

.. autoapisummary::

   avalanche.models.pnn.LinearAdapter
   avalanche.models.pnn.MLPAdapter
   avalanche.models.pnn.PNNColumn
   avalanche.models.pnn.PNNLayer
   avalanche.models.pnn.PNN



.. py:class:: LinearAdapter(in_features, out_features_per_column, num_prev_modules)

   Bases: :class:`torch.nn.Module`

   Base class for all neural network modules.

   Your models should also subclass this class.

   Modules can also contain other Modules, allowing to nest them in
   a tree structure. You can assign the submodules as regular attributes::

       import torch.nn as nn
       import torch.nn.functional as F

       class Model(nn.Module):
           def __init__(self):
               super(Model, self).__init__()
               self.conv1 = nn.Conv2d(1, 20, 5)
               self.conv2 = nn.Conv2d(20, 20, 5)

           def forward(self, x):
               x = F.relu(self.conv1(x))
               return F.relu(self.conv2(x))

   Submodules assigned in this way will be registered, and will have their
   parameters converted too when you call :meth:`to`, etc.

   :ivar training: Boolean represents whether this module is in training or
                   evaluation mode.
   :vartype training: bool

   Linear adapter for Progressive Neural Networks.

   :param in_features: size of each input sample
   :param out_features_per_column: size of each output sample
   :param num_prev_modules: number of previous modules

   .. method:: forward(self, x)



.. py:class:: MLPAdapter(in_features, out_features_per_column, num_prev_modules, activation=F.relu)

   Bases: :class:`torch.nn.Module`

   Base class for all neural network modules.

   Your models should also subclass this class.

   Modules can also contain other Modules, allowing to nest them in
   a tree structure. You can assign the submodules as regular attributes::

       import torch.nn as nn
       import torch.nn.functional as F

       class Model(nn.Module):
           def __init__(self):
               super(Model, self).__init__()
               self.conv1 = nn.Conv2d(1, 20, 5)
               self.conv2 = nn.Conv2d(20, 20, 5)

           def forward(self, x):
               x = F.relu(self.conv1(x))
               return F.relu(self.conv2(x))

   Submodules assigned in this way will be registered, and will have their
   parameters converted too when you call :meth:`to`, etc.

   :ivar training: Boolean represents whether this module is in training or
                   evaluation mode.
   :vartype training: bool

   MLP adapter for Progressive Neural Networks.

   :param in_features: size of each input sample
   :param out_features_per_column: size of each output sample
   :param num_prev_modules: number of previous modules
   :param activation: activation function (default=ReLU)

   .. method:: forward(self, x)



.. py:class:: PNNColumn(in_features, out_features_per_column, num_prev_modules, adapter='mlp')

   Bases: :class:`torch.nn.Module`

   Base class for all neural network modules.

   Your models should also subclass this class.

   Modules can also contain other Modules, allowing to nest them in
   a tree structure. You can assign the submodules as regular attributes::

       import torch.nn as nn
       import torch.nn.functional as F

       class Model(nn.Module):
           def __init__(self):
               super(Model, self).__init__()
               self.conv1 = nn.Conv2d(1, 20, 5)
               self.conv2 = nn.Conv2d(20, 20, 5)

           def forward(self, x):
               x = F.relu(self.conv1(x))
               return F.relu(self.conv2(x))

   Submodules assigned in this way will be registered, and will have their
   parameters converted too when you call :meth:`to`, etc.

   :ivar training: Boolean represents whether this module is in training or
                   evaluation mode.
   :vartype training: bool

   Progressive Neural Network column.

   :param in_features: size of each input sample
   :param out_features_per_column:
       size of each output sample (single column)
   :param num_prev_modules: number of previous columns
   :param adapter: adapter type. One of {'linear', 'mlp'} (default='mlp')

   .. method:: freeze(self)


   .. method:: forward(self, x)



.. py:class:: PNNLayer(in_features, out_features_per_column, adapter='mlp')

   Bases: :class:`avalanche.models.MultiTaskModule`

   Multi-task modules are `torch.nn.Modules`s for multi-task
   scenarios. The `forward` method accepts task labels, one for
   each sample in the mini-batch.

   By default the `forward` method splits the mini-batch by task
   and calls `forward_single_task`. Subclasses must implement
   `forward_single_task` or override `forward.

   if `task_labels == None`, the output is computed in parallel
   for each task.

   Progressive Neural Network layer.

   The adaptation phase assumes that each experience is a separate task.
   Multiple experiences with the same task label or multiple task labels
   within the same experience will result in a runtime error.

   :param in_features: size of each input sample
   :param out_features_per_column:
       size of each output sample (single column)
   :param adapter: adapter type. One of {'linear', 'mlp'} (default='mlp')

   .. method:: num_columns(self)
      :property:


   .. method:: train_adaptation(self, dataset: AvalancheDataset)

      Training adaptation for PNN layer.

      Adds an additional column to the layer.

      :param dataset:
      :return:


   .. method:: forward_single_task(self, x, task_label)

      Forward.

      :param x: list of inputs.
      :param task_label:
      :return:



.. py:class:: PNN(num_layers=1, in_features=784, hidden_features_per_column=100, adapter='mlp')

   Bases: :class:`avalanche.models.MultiTaskModule`

   Multi-task modules are `torch.nn.Modules`s for multi-task
   scenarios. The `forward` method accepts task labels, one for
   each sample in the mini-batch.

   By default the `forward` method splits the mini-batch by task
   and calls `forward_single_task`. Subclasses must implement
   `forward_single_task` or override `forward.

   if `task_labels == None`, the output is computed in parallel
   for each task.

   Progressive Neural Network.

   The model assumes that each experience is a separate task.
   Multiple experiences with the same task label or multiple task labels
   within the same experience will result in a runtime error.

   :param num_layers: number of layers (default=1)
   :param in_features: size of each input sample
   :param hidden_features_per_column:
       number of hidden units for each column
   :param adapter: adapter type. One of {'linear', 'mlp'} (default='mlp')

   .. method:: forward_single_task(self, x, task_label)

      Forward.

      :param x:
      :param task_label:
      :return:



