:mod:`avalanche.training.plugins.lwf`
=====================================

.. py:module:: avalanche.training.plugins.lwf


Module Contents
---------------

Classes
~~~~~~~

.. autoapisummary::

   avalanche.training.plugins.lwf.LwFPlugin



.. py:class:: LwFPlugin(alpha=1, temperature=2)

   Bases: :class:`avalanche.training.plugins.strategy_plugin.StrategyPlugin`

   A Learning without Forgetting plugin.
   LwF uses distillation to regularize the current loss with soft targets
   taken from a previous version of the model.
   This plugin does not use task identities.

   :param alpha: distillation hyperparameter. It can be either a float
           number or a list containing alpha for each experience.
   :param temperature: softmax temperature for distillation

   .. attribute:: prev_classes
      

      In Avalanche, targets of different experiences are not ordered. 
      As a result, some units may be allocated even though their 
      corresponding class has never been seen by the model.
      Knowledge distillation uses only units corresponding to old classes. 


   .. method:: penalty(self, out, x, alpha, curr_model)

      Compute weighted distillation loss.


   .. method:: before_backward(self, strategy, **kwargs)

      Add distillation loss


   .. method:: after_training_exp(self, strategy, **kwargs)

      Save a copy of the model after each experience and
      update self.prev_classes to include the newly learned classes.



