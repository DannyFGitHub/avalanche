<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>avalanche.benchmarks.generators &mdash; Avalanche 0.1 documentation</title>
      <link rel="stylesheet" href="../../../../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../../../../_static/css/mystyle.css" type="text/css" />
      <link rel="stylesheet" href="../../../../_static/graphviz.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../../../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="../../../../" id="documentation_options" src="../../../../_static/documentation_options.js"></script>
        <script src="../../../../_static/jquery.js"></script>
        <script src="../../../../_static/underscore.js"></script>
        <script src="../../../../_static/doctools.js"></script>
    <script src="../../../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../../../genindex/" />
    <link rel="search" title="Search" href="../../../../search/" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../../../../">
            <img src="../../../../_static/avalanche_logo.png" class="logo" alt="Logo"/>
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../../search/" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Avalanche API:</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="reference internal" href="../../../">API Reference</a><ul class="current">
<li class="toctree-l2 current"><a class="reference internal" href="../../"><code class="xref py py-mod docutils literal notranslate"><span class="pre">avalanche</span></code></a><ul class="current">
<li class="toctree-l3 current"><a class="reference internal" href="../../#subpackages">Subpackages</a><ul class="current">
<li class="toctree-l4 current"><a class="reference internal" href="../"><code class="xref py py-mod docutils literal notranslate"><span class="pre">avalanche.benchmarks</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../evaluation/"><code class="xref py py-mod docutils literal notranslate"><span class="pre">avalanche.evaluation</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../logging/"><code class="xref py py-mod docutils literal notranslate"><span class="pre">avalanche.logging</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../models/"><code class="xref py py-mod docutils literal notranslate"><span class="pre">avalanche.models</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../training/"><code class="xref py py-mod docutils literal notranslate"><span class="pre">avalanche.training</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../#submodules">Submodules</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../core/"><code class="xref py py-mod docutils literal notranslate"><span class="pre">avalanche.core</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../#package-contents">Package Contents</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../#avalanche.__version__">__version__</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../../">Avalanche</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../../../" class="icon icon-home"></a> &raquo;</li>
      <li><code class="xref py py-mod docutils literal notranslate"><span class="pre">avalanche.benchmarks.generators</span></code></li>
      <li class="wy-breadcrumbs-aside">
            <a href="../../../../_sources/autoapi/avalanche/benchmarks/generators/index.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="module-avalanche.benchmarks.generators">
<span id="avalanche-benchmarks-generators"></span><h1><a class="reference internal" href="#module-avalanche.benchmarks.generators" title="avalanche.benchmarks.generators"><code class="xref py py-mod docutils literal notranslate"><span class="pre">avalanche.benchmarks.generators</span></code></a><a class="headerlink" href="#module-avalanche.benchmarks.generators" title="Permalink to this headline"></a></h1>
<section id="submodules">
<h2>Submodules<a class="headerlink" href="#submodules" title="Permalink to this headline"></a></h2>
<div class="toctree-wrapper compound">
<ul>
<li class="toctree-l1"><a class="reference internal" href="benchmark_generators/"><code class="xref py py-mod docutils literal notranslate"><span class="pre">avalanche.benchmarks.generators.benchmark_generators</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="scenario_generators/"><code class="xref py py-mod docutils literal notranslate"><span class="pre">avalanche.benchmarks.generators.scenario_generators</span></code></a></li>
</ul>
</div>
</section>
<section id="package-contents">
<h2>Package Contents<a class="headerlink" href="#package-contents" title="Permalink to this headline"></a></h2>
<section id="functions">
<h3>Functions<a class="headerlink" href="#functions" title="Permalink to this headline"></a></h3>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#avalanche.benchmarks.generators.nc_scenario" title="avalanche.benchmarks.generators.nc_scenario"><code class="xref py py-obj docutils literal notranslate"><span class="pre">nc_scenario</span></code></a>(train_dataset: Union[Sequence[SupportedDataset], SupportedDataset], test_dataset: Union[Sequence[SupportedDataset], SupportedDataset], n_experiences: int, task_labels: bool, *, shuffle: bool = True, seed: Optional[int] = None, fixed_class_order: Sequence[int] = None, per_exp_classes: Dict[int, int] = None, class_ids_from_zero_from_first_exp: bool = False, class_ids_from_zero_in_each_exp: bool = False, one_dataset_per_exp: bool = False, reproducibility_data: Dict[str, Any] = None) → NCScenario</p></td>
<td><p>This helper function is DEPRECATED in favor of <cite>nc_benchmark</cite>.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#avalanche.benchmarks.generators.ni_scenario" title="avalanche.benchmarks.generators.ni_scenario"><code class="xref py py-obj docutils literal notranslate"><span class="pre">ni_scenario</span></code></a>(train_dataset: Union[Sequence[SupportedDataset], SupportedDataset], test_dataset: Union[Sequence[SupportedDataset], SupportedDataset], n_experiences: int, *, task_labels: bool = False, shuffle: bool = True, seed: Optional[int] = None, balance_experiences: bool = False, min_class_patterns_in_exp: int = 0, fixed_exp_assignment: Optional[Sequence[Sequence[int]]] = None, reproducibility_data: Optional[Dict[str, Any]] = None) → NIScenario</p></td>
<td><p>This helper function is DEPRECATED in favor of <cite>ni_benchmark</cite>.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#avalanche.benchmarks.generators.dataset_scenario" title="avalanche.benchmarks.generators.dataset_scenario"><code class="xref py py-obj docutils literal notranslate"><span class="pre">dataset_scenario</span></code></a>(train_dataset_list: Sequence[SupportedDataset], test_dataset_list: Sequence[SupportedDataset], task_labels: Sequence[int], *, complete_test_set_only: bool = False, dataset_type: AvalancheDatasetType = AvalancheDatasetType.UNDEFINED) → GenericCLScenario</p></td>
<td><p>This helper function is DEPRECATED in favor of <cite>dataset_benchmark</cite>.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#avalanche.benchmarks.generators.filelist_scenario" title="avalanche.benchmarks.generators.filelist_scenario"><code class="xref py py-obj docutils literal notranslate"><span class="pre">filelist_scenario</span></code></a>(root: Union[str, Path], train_file_lists: Sequence[Union[str, Path]], test_file_lists: Union[Union[str, Path], Sequence[Union[str, Path]]], task_labels: Sequence[int], *, complete_test_set_only: bool = False, train_transform=None, train_target_transform=None, eval_transform=None, eval_target_transform=None) → GenericCLScenario</p></td>
<td><p>This helper function is DEPRECATED in favor of <cite>filelist_benchmark</cite>.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#avalanche.benchmarks.generators.paths_scenario" title="avalanche.benchmarks.generators.paths_scenario"><code class="xref py py-obj docutils literal notranslate"><span class="pre">paths_scenario</span></code></a>(train_list_of_files: Sequence[Sequence[FileAndLabel]], test_list_of_files: Union[Sequence[FileAndLabel], Sequence[Sequence[FileAndLabel]]], task_labels: Sequence[int], *, complete_test_set_only: bool = False, train_transform=None, train_target_transform=None, eval_transform=None, eval_target_transform=None, dataset_type: AvalancheDatasetType = AvalancheDatasetType.UNDEFINED) → GenericCLScenario</p></td>
<td><p>This helper function is DEPRECATED in favor of <cite>paths_benchmark</cite>.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#avalanche.benchmarks.generators.tensors_scenario" title="avalanche.benchmarks.generators.tensors_scenario"><code class="xref py py-obj docutils literal notranslate"><span class="pre">tensors_scenario</span></code></a>(train_tensors: Sequence[Sequence[Any]], test_tensors: Sequence[Sequence[Any]], task_labels: Sequence[int], *, complete_test_set_only: bool = False, train_transform=None, train_target_transform=None, eval_transform=None, eval_target_transform=None, dataset_type: AvalancheDatasetType = AvalancheDatasetType.UNDEFINED) → GenericCLScenario</p></td>
<td><p>This helper function is DEPRECATED in favor of <cite>tensors_benchmark</cite>.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#avalanche.benchmarks.generators.tensor_scenario" title="avalanche.benchmarks.generators.tensor_scenario"><code class="xref py py-obj docutils literal notranslate"><span class="pre">tensor_scenario</span></code></a>(train_data_x: Sequence[Any], train_data_y: Sequence[Sequence[SupportsInt]], test_data_x: Union[Any, Sequence[Any]], test_data_y: Union[Any, Sequence[Sequence[SupportsInt]]], task_labels: Sequence[int], *, complete_test_set_only: bool = False, train_transform=None, train_target_transform=None, eval_transform=None, eval_target_transform=None, dataset_type: AvalancheDatasetType = AvalancheDatasetType.UNDEFINED) → GenericCLScenario</p></td>
<td><p>This helper function is DEPRECATED in favor of <cite>tensors_benchmark</cite>.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#avalanche.benchmarks.generators.nc_benchmark" title="avalanche.benchmarks.generators.nc_benchmark"><code class="xref py py-obj docutils literal notranslate"><span class="pre">nc_benchmark</span></code></a>(train_dataset: Union[Sequence[SupportedDataset], SupportedDataset], test_dataset: Union[Sequence[SupportedDataset], SupportedDataset], n_experiences: int, task_labels: bool, *, shuffle: bool = True, seed: Optional[int] = None, fixed_class_order: Sequence[int] = None, per_exp_classes: Dict[int, int] = None, class_ids_from_zero_from_first_exp: bool = False, class_ids_from_zero_in_each_exp: bool = False, one_dataset_per_exp: bool = False, train_transform=None, eval_transform=None, reproducibility_data: Dict[str, Any] = None) → NCScenario</p></td>
<td><p>This is the high-level benchmark instances generator for the</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#avalanche.benchmarks.generators.ni_benchmark" title="avalanche.benchmarks.generators.ni_benchmark"><code class="xref py py-obj docutils literal notranslate"><span class="pre">ni_benchmark</span></code></a>(train_dataset: Union[Sequence[SupportedDataset], SupportedDataset], test_dataset: Union[Sequence[SupportedDataset], SupportedDataset], n_experiences: int, *, task_labels: bool = False, shuffle: bool = True, seed: Optional[int] = None, balance_experiences: bool = False, min_class_patterns_in_exp: int = 0, fixed_exp_assignment: Optional[Sequence[Sequence[int]]] = None, train_transform=None, eval_transform=None, reproducibility_data: Optional[Dict[str, Any]] = None) → NIScenario</p></td>
<td><p>This is the high-level benchmark instances generator for the</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#avalanche.benchmarks.generators.data_incremental_benchmark" title="avalanche.benchmarks.generators.data_incremental_benchmark"><code class="xref py py-obj docutils literal notranslate"><span class="pre">data_incremental_benchmark</span></code></a>(benchmark_instance: GenericCLScenario, experience_size: int, shuffle: bool = False, drop_last: bool = False, split_streams: Sequence[str] = ('train', ), custom_split_strategy: Callable[[Experience], Sequence[AvalancheDataset]] = None, experience_factory: Callable[[GenericScenarioStream, int], Experience] = None)</p></td>
<td><p>High-level benchmark generator for a Data Incremental setup.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#avalanche.benchmarks.generators.benchmark_with_validation_stream" title="avalanche.benchmarks.generators.benchmark_with_validation_stream"><code class="xref py py-obj docutils literal notranslate"><span class="pre">benchmark_with_validation_stream</span></code></a>(benchmark_instance: GenericCLScenario, validation_size: Union[int, float], shuffle: bool = False, input_stream: str = 'train', output_stream: str = 'valid', custom_split_strategy: Callable[[Experience], Tuple[AvalancheDataset, AvalancheDataset]] = None, *, experience_factory: Callable[[GenericScenarioStream, int], Experience] = None, lazy_splitting: bool = None)</p></td>
<td><p>Helper that can be used to obtain a benchmark with a validation stream.</p></td>
</tr>
</tbody>
</table>
<dl class="py function">
<dt class="sig sig-object py" id="avalanche.benchmarks.generators.nc_scenario">
<span class="sig-prename descclassname"><span class="pre">avalanche.benchmarks.generators.</span></span><span class="sig-name descname"><span class="pre">nc_scenario</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">train_dataset</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">Sequence</span><span class="p"><span class="pre">[</span></span><span class="pre">SupportedDataset</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span> </span><span class="pre">SupportedDataset</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">test_dataset</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">Sequence</span><span class="p"><span class="pre">[</span></span><span class="pre">SupportedDataset</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span> </span><span class="pre">SupportedDataset</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_experiences</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">task_labels</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">bool</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">shuffle</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">bool</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">seed</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">fixed_class_order</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Sequence</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">per_exp_classes</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span> </span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">class_ids_from_zero_from_first_exp</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">bool</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">class_ids_from_zero_in_each_exp</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">bool</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">one_dataset_per_exp</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">bool</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">reproducibility_data</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">NCScenario</span></span></span><a class="reference internal" href="../../../../_modules/avalanche/benchmarks/generators/scenario_generators/#nc_scenario"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.benchmarks.generators.nc_scenario" title="Permalink to this definition"></a></dt>
<dd><p>This helper function is DEPRECATED in favor of <cite>nc_benchmark</cite>.</p>
<p>This method is the high-level specific scenario generator for the
“New Classes” (NC) case. Given a sequence of train and test datasets creates
the continual stream of data as a series of experiences. Each experience
will contain all the patterns belonging to a certain set of classes and a
class won’t be assigned to more than one experience.</p>
<p>The <code class="docutils literal notranslate"><span class="pre">task_labels</span></code> parameter determines if each incremental experience has
an increasing task label or if, at the contrary, a default task label “0”
has to be assigned to all experiences. This can be useful when
differentiating between Single-Incremental-Task and Multi-Task scenarios.</p>
<p>There are other important parameters that can be specified in order to tweak
the behaviour of the resulting scenario. Please take a few minutes to read
and understand them as they may save you a lot of work.</p>
<p>This generator features a integrated reproducibility mechanism that allows
the user to store and later re-load a scenario. For more info see the
<code class="docutils literal notranslate"><span class="pre">reproducibility_data</span></code> parameter.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>train_dataset</strong> – A list of training datasets, or a single dataset.</p></li>
<li><p><strong>test_dataset</strong> – A list of test datasets, or a single test dataset.</p></li>
<li><p><strong>n_experiences</strong> – The number of incremental experience. This is not used
when using multiple train/test datasets with the <code class="docutils literal notranslate"><span class="pre">one_dataset_per_exp</span></code>
parameter set to True.</p></li>
<li><p><strong>task_labels</strong> – If True, each experience will have an ascending task
label. If False, the task label will be 0 for all the experiences.</p></li>
<li><p><strong>shuffle</strong> – If True, the class (or experience) order will be shuffled.
Defaults to True.</p></li>
<li><p><strong>seed</strong> – If <code class="docutils literal notranslate"><span class="pre">shuffle</span></code> is True and seed is not None, the class (or
experience) order will be shuffled according to the seed. When None, the
current PyTorch random number generator state will be used. Defaults to
None.</p></li>
<li><p><strong>fixed_class_order</strong> – If not None, the class order to use (overrides
the shuffle argument). Very useful for enhancing reproducibility.
Defaults to None.</p></li>
<li><p><strong>per_exp_classes</strong> – Is not None, a dictionary whose keys are
(0-indexed) experience IDs and their values are the number of classes
to include in the respective experiences. The dictionary doesn’t
have to contain a key for each experience! All the remaining experiences
will contain an equal amount of the remaining classes. The
remaining number of classes must be divisible without remainder
by the remaining number of experiences. For instance,
if you want to include 50 classes in the first experience
while equally distributing remaining classes across remaining
experiences, just pass the “{0: 50}” dictionary as the
per_experience_classes parameter. Defaults to None.</p></li>
<li><p><strong>class_ids_from_zero_from_first_exp</strong> – If True, original class IDs
will be remapped so that they will appear as having an ascending
order. For instance, if the resulting class order after shuffling
(or defined by fixed_class_order) is [23, 34, 11, 7, 6, …] and
class_ids_from_zero_from_first_exp is True, then all the patterns
belonging to class 23 will appear as belonging to class “0”,
class “34” will be mapped to “1”, class “11” to “2” and so on.
This is very useful when drawing confusion matrices and when dealing
with algorithms with dynamic head expansion. Defaults to False.
Mutually exclusive with the <code class="docutils literal notranslate"><span class="pre">class_ids_from_zero_in_each_exp</span></code>
parameter.</p></li>
<li><p><strong>class_ids_from_zero_in_each_exp</strong> – If True, original class IDs
will be mapped to range [0, n_classes_in_exp) for each experience.
Defaults to False. Mutually exclusive with the
<code class="docutils literal notranslate"><span class="pre">class_ids_from_zero_from_first_exp</span></code> parameter.</p></li>
<li><p><strong>one_dataset_per_exp</strong> – available only when multiple train-test
datasets are provided. If True, each dataset will be treated as a
experience. Mutually exclusive with the <code class="docutils literal notranslate"><span class="pre">per_experience_classes</span></code> and
<code class="docutils literal notranslate"><span class="pre">fixed_class_order</span></code> parameters. Overrides the <code class="docutils literal notranslate"><span class="pre">n_experiences</span></code>
parameter. Defaults to False.</p></li>
<li><p><strong>reproducibility_data</strong> – If not None, overrides all the other
scenario definition options. This is usually a dictionary containing
data used to reproduce a specific experiment. One can use the
<code class="docutils literal notranslate"><span class="pre">get_reproducibility_data</span></code> method to get (and even distribute)
the experiment setup so that it can be loaded by passing it as this
parameter. In this way one can be sure that the same specific
experimental setup is being used (for reproducibility purposes).
Beware that, in order to reproduce an experiment, the same train and
test datasets must be used. Defaults to None.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A properly initialized <code class="xref py py-class docutils literal notranslate"><span class="pre">NCScenario</span></code> instance.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="avalanche.benchmarks.generators.ni_scenario">
<span class="sig-prename descclassname"><span class="pre">avalanche.benchmarks.generators.</span></span><span class="sig-name descname"><span class="pre">ni_scenario</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">train_dataset</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">Sequence</span><span class="p"><span class="pre">[</span></span><span class="pre">SupportedDataset</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span> </span><span class="pre">SupportedDataset</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">test_dataset</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">Sequence</span><span class="p"><span class="pre">[</span></span><span class="pre">SupportedDataset</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span> </span><span class="pre">SupportedDataset</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_experiences</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">task_labels</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">bool</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">shuffle</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">bool</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">seed</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">balance_experiences</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">bool</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">min_class_patterns_in_exp</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">fixed_exp_assignment</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">Sequence</span><span class="p"><span class="pre">[</span></span><span class="pre">Sequence</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">reproducibility_data</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">NIScenario</span></span></span><a class="reference internal" href="../../../../_modules/avalanche/benchmarks/generators/scenario_generators/#ni_scenario"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.benchmarks.generators.ni_scenario" title="Permalink to this definition"></a></dt>
<dd><p>This helper function is DEPRECATED in favor of <cite>ni_benchmark</cite>.</p>
<p>This method is the high-level specific scenario generator for the
“New Instances” (NI) case. Given a sequence of train and test datasets
creates the continual stream of data as a series of experiences. Each
experience will contain patterns belonging to the same classes.</p>
<p>The <code class="docutils literal notranslate"><span class="pre">task_labels</span></code> parameter determines if each incremental experience has
an increasing task label or if, at the contrary, a default task label “0”
has to be assigned to all experiences. This can be useful when
differentiating between Single-Incremental-Task and Multi-Task scenarios.</p>
<p>There are other important parameters that can be specified in order to tweak
the behaviour of the resulting scenario. Please take a few minutes to read
and understand them as they may save you a lot of work.</p>
<p>This generator features an integrated reproducibility mechanism that allows
the user to store and later re-load a scenario. For more info see the
<code class="docutils literal notranslate"><span class="pre">reproducibility_data</span></code> parameter.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>train_dataset</strong> – A list of training datasets, or a single dataset.</p></li>
<li><p><strong>test_dataset</strong> – A list of test datasets, or a single test dataset.</p></li>
<li><p><strong>n_experiences</strong> – The number of experiences.</p></li>
<li><p><strong>task_labels</strong> – If True, each experience will have an ascending task
label. If False, the task label will be 0 for all the experiences.</p></li>
<li><p><strong>shuffle</strong> – If True, patterns order will be shuffled.</p></li>
<li><p><strong>seed</strong> – A valid int used to initialize the random number generator.
Can be None.</p></li>
<li><p><strong>balance_experiences</strong> – If True, pattern of each class will be equally
spread across all experiences. If False, patterns will be assigned to
experiences in a complete random way. Defaults to False.</p></li>
<li><p><strong>min_class_patterns_in_exp</strong> – The minimum amount of patterns of
every class that must be assigned to every experience. Compatible with
the <code class="docutils literal notranslate"><span class="pre">balance_experiences</span></code> parameter. An exception will be raised if
this constraint can’t be satisfied. Defaults to 0.</p></li>
<li><p><strong>fixed_exp_assignment</strong> – If not None, the pattern assignment
to use. It must be a list with an entry for each experience. Each entry
is a list that contains the indexes of patterns belonging to that
experience. Overrides the <code class="docutils literal notranslate"><span class="pre">shuffle</span></code>, <code class="docutils literal notranslate"><span class="pre">balance_experiences</span></code> and
<code class="docutils literal notranslate"><span class="pre">min_class_patterns_in_exp</span></code> parameters.</p></li>
<li><p><strong>reproducibility_data</strong> – If not None, overrides all the other
scenario definition options, including <code class="docutils literal notranslate"><span class="pre">fixed_exp_assignment</span></code>.
This is usually a dictionary containing data used to
reproduce a specific experiment. One can use the
<code class="docutils literal notranslate"><span class="pre">get_reproducibility_data</span></code> method to get (and even distribute)
the experiment setup so that it can be loaded by passing it as this
parameter. In this way one can be sure that the same specific
experimental setup is being used (for reproducibility purposes).
Beware that, in order to reproduce an experiment, the same train and
test datasets must be used. Defaults to None.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A properly initialized <code class="xref py py-class docutils literal notranslate"><span class="pre">NIScenario</span></code> instance.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="avalanche.benchmarks.generators.dataset_scenario">
<span class="sig-prename descclassname"><span class="pre">avalanche.benchmarks.generators.</span></span><span class="sig-name descname"><span class="pre">dataset_scenario</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">train_dataset_list</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Sequence</span><span class="p"><span class="pre">[</span></span><span class="pre">SupportedDataset</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">test_dataset_list</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Sequence</span><span class="p"><span class="pre">[</span></span><span class="pre">SupportedDataset</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">task_labels</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Sequence</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">complete_test_set_only</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">bool</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dataset_type</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">AvalancheDatasetType</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">AvalancheDatasetType.UNDEFINED</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">GenericCLScenario</span></span></span><a class="reference internal" href="../../../../_modules/avalanche/benchmarks/generators/scenario_generators/#dataset_scenario"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.benchmarks.generators.dataset_scenario" title="Permalink to this definition"></a></dt>
<dd><p>This helper function is DEPRECATED in favor of <cite>dataset_benchmark</cite>.</p>
<p>Creates a generic scenario given a list of datasets and the respective task
labels. Each training dataset will be considered as a separate training
experience. Contents of the datasets will not be changed, including the
targets.</p>
<p>When loading the datasets from a set of fixed file lists, consider using
the <a class="reference internal" href="#avalanche.benchmarks.generators.filelist_scenario" title="avalanche.benchmarks.generators.filelist_scenario"><code class="xref py py-func docutils literal notranslate"><span class="pre">filelist_scenario()</span></code></a> helper method instead. Also, loading from
a list of paths is supported through the <a class="reference internal" href="#avalanche.benchmarks.generators.paths_scenario" title="avalanche.benchmarks.generators.paths_scenario"><code class="xref py py-func docutils literal notranslate"><span class="pre">paths_scenario()</span></code></a> helper.</p>
<p>In its base form, this function accepts a list of test datasets that must
contain the same amount of datasets of the training list.
Those pairs are then used to create the “past”, “cumulative”
(a.k.a. growing) and “future” test sets. However, in certain Continual
Learning scenarios only the concept of “complete” test set makes sense. In
that case, the <code class="docutils literal notranslate"><span class="pre">complete_test_set_only</span></code> parameter should be set to True
(see the parameter description for more info).</p>
<p>Beware that pattern transformations must already be included in the
datasets (when needed).</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>train_dataset_list</strong> – A list of training datasets.</p></li>
<li><p><strong>test_dataset_list</strong> – A list of test datasets.</p></li>
<li><p><strong>task_labels</strong> – A list of task labels. Must contain the same amount of
elements of the <code class="docutils literal notranslate"><span class="pre">train_dataset_list</span></code> parameter. For
Single-Incremental-Task (a.k.a. Task-Free) scenarios, this is usually
a list of zeros. For Multi Task scenario, this is usually a list of
ascending task labels (starting from 0).</p></li>
<li><p><strong>complete_test_set_only</strong> – If True, only the complete test set will
be returned by the scenario. This means that the <code class="docutils literal notranslate"><span class="pre">test_dataset_list</span></code>
parameter must be list with a single element (the complete test set).
Defaults to False, which means that <code class="docutils literal notranslate"><span class="pre">train_dataset_list</span></code> and
<code class="docutils literal notranslate"><span class="pre">test_dataset_list</span></code> must contain the same amount of datasets.</p></li>
<li><p><strong>dataset_type</strong> – The type of the dataset. Defaults to None, which
means that the type will be obtained from the input datasets. If input
datasets are not instances of <code class="xref py py-class docutils literal notranslate"><span class="pre">AvalancheDataset</span></code>, the type
UNDEFINED will be used.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A properly initialized <code class="xref py py-class docutils literal notranslate"><span class="pre">GenericCLScenario</span></code> instance.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="avalanche.benchmarks.generators.filelist_scenario">
<span class="sig-prename descclassname"><span class="pre">avalanche.benchmarks.generators.</span></span><span class="sig-name descname"><span class="pre">filelist_scenario</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">root</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span> </span><span class="pre">Path</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">train_file_lists</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Sequence</span><span class="p"><span class="pre">[</span></span><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span> </span><span class="pre">Path</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">test_file_lists</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span> </span><span class="pre">Path</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span> </span><span class="pre">Sequence</span><span class="p"><span class="pre">[</span></span><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span> </span><span class="pre">Path</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">task_labels</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Sequence</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">complete_test_set_only</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">bool</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">train_transform</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">train_target_transform</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">eval_transform</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">eval_target_transform</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">GenericCLScenario</span></span></span><a class="reference internal" href="../../../../_modules/avalanche/benchmarks/generators/scenario_generators/#filelist_scenario"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.benchmarks.generators.filelist_scenario" title="Permalink to this definition"></a></dt>
<dd><p>This helper function is DEPRECATED in favor of <cite>filelist_benchmark</cite>.</p>
<p>Creates a generic scenario given a list of filelists and the respective task
labels. A separate dataset will be created for each filelist and each of
those training datasets will be considered a separate training experience.</p>
<p>In its base form, this function accepts a list of filelists for the test
datsets that must contain the same amount of elements of the training list.
Those pairs of datasets are then used to create the “past”, “cumulative”
(a.k.a. growing) and “future” test sets. However, in certain Continual
Learning scenarios only the concept of “complete” test set makes sense. In
that case, the <code class="docutils literal notranslate"><span class="pre">complete_test_set_only</span></code> should be set to True (see the
parameter description for more info).</p>
<p>This helper functions is the best shot when loading Caffe-style dataset
based on filelists.</p>
<p>The resulting benchmark instance and the intermediate datasets used to
populate it will be of type CLASSIFICATION.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>root</strong> – The root path of the dataset.</p></li>
<li><p><strong>train_file_lists</strong> – A list of filelists describing the
paths of the training patterns for each experience.</p></li>
<li><p><strong>test_file_lists</strong> – A list of filelists describing the
paths of the test patterns for each experience.</p></li>
<li><p><strong>task_labels</strong> – A list of task labels. Must contain the same amount of
elements of the <code class="docutils literal notranslate"><span class="pre">train_file_lists</span></code> parameter. For
Single-Incremental-Task (a.k.a. Task-Free) scenarios, this is usually
a list of zeros. For Multi Task scenario, this is usually a list of
ascending task labels (starting from 0).</p></li>
<li><p><strong>complete_test_set_only</strong> – If True, only the complete test set will
be returned by the scenario. This means that the <code class="docutils literal notranslate"><span class="pre">test_file_lists</span></code>
parameter must be list with a single element (the complete test set).
Alternatively, can be a plain string or <code class="xref py py-class docutils literal notranslate"><span class="pre">Path</span></code> object.
Defaults to False, which means that <code class="docutils literal notranslate"><span class="pre">train_file_lists</span></code> and
<code class="docutils literal notranslate"><span class="pre">test_file_lists</span></code> must contain the same amount of filelists paths.</p></li>
<li><p><strong>train_transform</strong> – The transformation to apply to the training data,
e.g. a random crop, a normalization or a concatenation of different
transformations (see torchvision.transform documentation for a
comprehensive list of possible transformations). Defaults to None.</p></li>
<li><p><strong>train_target_transform</strong> – The transformation to apply to training
patterns targets. Defaults to None.</p></li>
<li><p><strong>eval_transform</strong> – The transformation to apply to the test data,
e.g. a random crop, a normalization or a concatenation of different
transformations (see torchvision.transform documentation for a
comprehensive list of possible transformations). Defaults to None.</p></li>
<li><p><strong>eval_target_transform</strong> – The transformation to apply to test
patterns targets. Defaults to None.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A properly initialized <code class="xref py py-class docutils literal notranslate"><span class="pre">GenericCLScenario</span></code> instance.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="avalanche.benchmarks.generators.paths_scenario">
<span class="sig-prename descclassname"><span class="pre">avalanche.benchmarks.generators.</span></span><span class="sig-name descname"><span class="pre">paths_scenario</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">train_list_of_files</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Sequence</span><span class="p"><span class="pre">[</span></span><span class="pre">Sequence</span><span class="p"><span class="pre">[</span></span><span class="pre">FileAndLabel</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">test_list_of_files</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">Sequence</span><span class="p"><span class="pre">[</span></span><span class="pre">FileAndLabel</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span> </span><span class="pre">Sequence</span><span class="p"><span class="pre">[</span></span><span class="pre">Sequence</span><span class="p"><span class="pre">[</span></span><span class="pre">FileAndLabel</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">task_labels</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Sequence</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">complete_test_set_only</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">bool</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">train_transform</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">train_target_transform</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">eval_transform</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">eval_target_transform</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dataset_type</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">AvalancheDatasetType</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">AvalancheDatasetType.UNDEFINED</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">GenericCLScenario</span></span></span><a class="reference internal" href="../../../../_modules/avalanche/benchmarks/generators/scenario_generators/#paths_scenario"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.benchmarks.generators.paths_scenario" title="Permalink to this definition"></a></dt>
<dd><p>This helper function is DEPRECATED in favor of <cite>paths_benchmark</cite>.</p>
<p>Creates a generic scenario given a list of files and class labels.
A separate dataset will be created for each list and each of
those training datasets will be considered a separate training experience.</p>
<p>This is very similar to <cite>filelist_scenario</cite>, with the main difference being
that <cite>filelist_scenario</cite> accepts, for each experience, a file list formatted
in Caffe-style. On the contrary, this accepts a list of tuples where each
tuple contains two elements: the full path to the pattern and its label.
Optionally, the tuple may contain a third element describing the bounding
box of the element to crop. This last bounding box may be useful when trying
to extract the part of the image depicting the desired element.</p>
<p>In its base form, this function accepts a list of lists of tuples for the
test datsets that must contain the same amount of lists of the training
list. Those pairs of datasets are then used to create the “past”,
“cumulative” (a.k.a. growing) and “future” test sets. However, in certain
Continual Learning scenarios only the concept of “complete” test set makes
sense. In that case, the <code class="docutils literal notranslate"><span class="pre">complete_test_set_only</span></code> should be set to True
(see the parameter description for more info).</p>
<p>The label of each pattern doesn’t have to be an int.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>train_list_of_files</strong> – A list of lists. Each list describes the paths
and labels of patterns to include in that training experience as tuples.
Each tuple must contain two elements: the full path to the pattern
and its class label. Optionally, the tuple may contain a third element
describing the bounding box to use for cropping (top, left, height,
width).</p></li>
<li><p><strong>test_list_of_files</strong> – A list of lists. Each list describes the paths
and labels of patterns to include in that test experience as tuples.
Each tuple must contain two elements: the full path to the pattern
and its class label. Optionally, the tuple may contain a third element
describing the bounding box to use for cropping (top, left, height,
width).</p></li>
<li><p><strong>task_labels</strong> – A list of task labels. Must contain the same amount of
elements of the <code class="docutils literal notranslate"><span class="pre">train_file_lists</span></code> parameter. For
Single-Incremental-Task (a.k.a. Task-Free) scenarios, this is usually
a list of zeros. For Multi Task scenario, this is usually a list of
ascending task labels (starting from 0).</p></li>
<li><p><strong>complete_test_set_only</strong> – If True, only the complete test set will
be returned by the scenario. This means that the <code class="docutils literal notranslate"><span class="pre">test_file_lists</span></code>
parameter must be list with a single element (the complete test set).
Alternatively, can be a plain string or <code class="xref py py-class docutils literal notranslate"><span class="pre">Path</span></code> object.
Defaults to False, which means that <code class="docutils literal notranslate"><span class="pre">train_file_lists</span></code> and
<code class="docutils literal notranslate"><span class="pre">test_file_lists</span></code> must contain the same amount of filelists paths.</p></li>
<li><p><strong>train_transform</strong> – The transformation to apply to the training data,
e.g. a random crop, a normalization or a concatenation of different
transformations (see torchvision.transform documentation for a
comprehensive list of possible transformations). Defaults to None.</p></li>
<li><p><strong>train_target_transform</strong> – The transformation to apply to training
patterns targets. Defaults to None.</p></li>
<li><p><strong>eval_transform</strong> – The transformation to apply to the test data,
e.g. a random crop, a normalization or a concatenation of different
transformations (see torchvision.transform documentation for a
comprehensive list of possible transformations). Defaults to None.</p></li>
<li><p><strong>eval_target_transform</strong> – The transformation to apply to test
patterns targets. Defaults to None.</p></li>
<li><p><strong>dataset_type</strong> – The type of the dataset. Defaults to UNDEFINED.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A properly initialized <code class="xref py py-class docutils literal notranslate"><span class="pre">GenericCLScenario</span></code> instance.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="avalanche.benchmarks.generators.tensors_scenario">
<span class="sig-prename descclassname"><span class="pre">avalanche.benchmarks.generators.</span></span><span class="sig-name descname"><span class="pre">tensors_scenario</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">train_tensors</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Sequence</span><span class="p"><span class="pre">[</span></span><span class="pre">Sequence</span><span class="p"><span class="pre">[</span></span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">test_tensors</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Sequence</span><span class="p"><span class="pre">[</span></span><span class="pre">Sequence</span><span class="p"><span class="pre">[</span></span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">task_labels</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Sequence</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">complete_test_set_only</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">bool</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">train_transform</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">train_target_transform</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">eval_transform</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">eval_target_transform</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dataset_type</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">AvalancheDatasetType</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">AvalancheDatasetType.UNDEFINED</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">GenericCLScenario</span></span></span><a class="reference internal" href="../../../../_modules/avalanche/benchmarks/generators/scenario_generators/#tensors_scenario"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.benchmarks.generators.tensors_scenario" title="Permalink to this definition"></a></dt>
<dd><p>This helper function is DEPRECATED in favor of <cite>tensors_benchmark</cite>.</p>
<p>Creates a generic scenario given lists of Tensors and the respective task
labels. A separate dataset will be created from each Tensor tuple
(x, y, …) and each of those training datasets will be considered a
separate training experience. Using this helper function is the lowest-level
way to create a Continual Learning scenario. When possible, consider using
higher level helpers.</p>
<p>Experiences are defined by passing lists of tensors as the <cite>train_tensors</cite>
and <cite>test_tensors</cite> parameter. Those parameters must be lists containing
sub-lists of tensors, one for each experience. Each tensor defines the value
of a feature (“x”, “y”, “z”, …) for all patterns of that experience.</p>
<p>By default the second tensor of each experience will be used to fill the
<cite>targets</cite> value (label of each pattern).</p>
<p>In its base form, the test lists must contain the same amount of elements of
the training lists. Those pairs of datasets are then used to create the
“past”, “cumulative” (a.k.a. growing) and “future” test sets.
However, in certain Continual Learning scenarios only the concept of
“complete” test set makes sense. In that case, the
<code class="docutils literal notranslate"><span class="pre">complete_test_set_only</span></code> should be set to True (see the parameter
description for more info).</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>train_tensors</strong> – A list of lists. The first list must contain the
tensors for the first training experience (one tensor per feature), the
second list must contain the tensors for the second training experience,
and so on.</p></li>
<li><p><strong>test_tensors</strong> – A list of lists. The first list must contain the
tensors for the first test experience (one tensor per feature), the
second list must contain the tensors for the second test experience,
and so on.</p></li>
<li><p><strong>task_labels</strong> – A list of task labels. Must contain a task label for
each experience. For Single-Incremental-Task (a.k.a. Task-Free)
scenarios, this is usually a list of zeros. For Multi Task scenario,
this is usually a list of ascending task labels (starting from 0).</p></li>
<li><p><strong>complete_test_set_only</strong> – If True, only the complete test set will
be returned by the scenario. This means that <code class="docutils literal notranslate"><span class="pre">test_tensors</span></code> must
define a single experience. Defaults to False, which means that
<code class="docutils literal notranslate"><span class="pre">train_tensors</span></code> and <code class="docutils literal notranslate"><span class="pre">test_tensors</span></code> must define the same
amount of experiences.</p></li>
<li><p><strong>train_transform</strong> – The transformation to apply to the training data,
e.g. a random crop, a normalization or a concatenation of different
transformations (see torchvision.transform documentation for a
comprehensive list of possible transformations). Defaults to None.</p></li>
<li><p><strong>train_target_transform</strong> – The transformation to apply to training
patterns targets. Defaults to None.</p></li>
<li><p><strong>eval_transform</strong> – The transformation to apply to the test data,
e.g. a random crop, a normalization or a concatenation of different
transformations (see torchvision.transform documentation for a
comprehensive list of possible transformations). Defaults to None.</p></li>
<li><p><strong>eval_target_transform</strong> – The transformation to apply to test
patterns targets. Defaults to None.</p></li>
<li><p><strong>dataset_type</strong> – The type of the dataset. Defaults to UNDEFINED.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A properly initialized <code class="xref py py-class docutils literal notranslate"><span class="pre">GenericCLScenario</span></code> instance.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="avalanche.benchmarks.generators.tensor_scenario">
<span class="sig-prename descclassname"><span class="pre">avalanche.benchmarks.generators.</span></span><span class="sig-name descname"><span class="pre">tensor_scenario</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">train_data_x</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Sequence</span><span class="p"><span class="pre">[</span></span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">train_data_y</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Sequence</span><span class="p"><span class="pre">[</span></span><span class="pre">Sequence</span><span class="p"><span class="pre">[</span></span><span class="pre">SupportsInt</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">test_data_x</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">Any</span><span class="p"><span class="pre">,</span> </span><span class="pre">Sequence</span><span class="p"><span class="pre">[</span></span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">test_data_y</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">Any</span><span class="p"><span class="pre">,</span> </span><span class="pre">Sequence</span><span class="p"><span class="pre">[</span></span><span class="pre">Sequence</span><span class="p"><span class="pre">[</span></span><span class="pre">SupportsInt</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">task_labels</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Sequence</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">complete_test_set_only</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">bool</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">train_transform</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">train_target_transform</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">eval_transform</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">eval_target_transform</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dataset_type</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">AvalancheDatasetType</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">AvalancheDatasetType.UNDEFINED</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">GenericCLScenario</span></span></span><a class="reference internal" href="../../../../_modules/avalanche/benchmarks/generators/scenario_generators/#tensor_scenario"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.benchmarks.generators.tensor_scenario" title="Permalink to this definition"></a></dt>
<dd><p>This helper function is DEPRECATED in favor of <cite>tensors_benchmark</cite>.</p>
<p>Please consider using <a class="reference internal" href="#avalanche.benchmarks.generators.tensors_benchmark" title="avalanche.benchmarks.generators.tensors_benchmark"><code class="xref py py-func docutils literal notranslate"><span class="pre">tensors_benchmark()</span></code></a> instead. When switching to
the new function, please keep in mind that the format of the parameters is
completely different!</p>
<p>Creates a generic scenario given lists of Tensors and the respective task
labels. A separate dataset will be created from each Tensor pair (x + y)
and each of those training datasets will be considered a separate
training experience. Contents of the datasets will not be changed, including
the targets. Using this helper function is the lower level way to create a
Continual Learning scenario. When possible, consider using higher level
helpers.</p>
<p>By default the second tensor of each experience will be used to fill the
<cite>targets</cite> value (label of each pattern).</p>
<p>In its base form, the test lists must contain the same amount of elements of
the training lists. Those pairs of datasets are then used to create the
“past”, “cumulative” (a.k.a. growing) and “future” test sets.
However, in certain Continual Learning scenarios only the concept of
“complete” test set makes sense. In that case, the
<code class="docutils literal notranslate"><span class="pre">complete_test_set_only</span></code> should be set to True (see the parameter
description for more info).</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>train_data_x</strong> – A list of Tensors (one per experience) containing the
patterns of the training sets.</p></li>
<li><p><strong>train_data_y</strong> – A list of Tensors or int lists containing the
labels of the patterns of the training sets. Must contain the same
number of elements of <code class="docutils literal notranslate"><span class="pre">train_datasets_x</span></code>.</p></li>
<li><p><strong>test_data_x</strong> – A Tensor or a list of Tensors (one per experience)
containing the patterns of the test sets.</p></li>
<li><p><strong>test_data_y</strong> – A Tensor or a list of Tensors or int lists containing
the labels of the patterns of the test sets. Must contain the same
number of elements of <code class="docutils literal notranslate"><span class="pre">test_datasets_x</span></code>.</p></li>
<li><p><strong>task_labels</strong> – A list of task labels. Must contain the same amount of
elements of the <code class="docutils literal notranslate"><span class="pre">train_datasets_x</span></code> parameter. For
Single-Incremental-Task (a.k.a. Task-Free) scenarios, this is usually
a list of zeros. For Multi Task scenario, this is usually a list of
ascending task labels (starting from 0).</p></li>
<li><p><strong>complete_test_set_only</strong> – If True, only the complete test set will
be returned by the scenario. This means that the <code class="docutils literal notranslate"><span class="pre">test_datasets_x</span></code> and
<code class="docutils literal notranslate"><span class="pre">test_datasets_y</span></code> parameters must be lists with a single element
(the complete test set). Defaults to False, which means that
<code class="docutils literal notranslate"><span class="pre">train_file_lists</span></code> and <code class="docutils literal notranslate"><span class="pre">test_file_lists</span></code> must contain the same
amount of filelists paths.</p></li>
<li><p><strong>train_transform</strong> – The transformation to apply to the training data,
e.g. a random crop, a normalization or a concatenation of different
transformations (see torchvision.transform documentation for a
comprehensive list of possible transformations). Defaults to None.</p></li>
<li><p><strong>train_target_transform</strong> – The transformation to apply to training
patterns targets. Defaults to None.</p></li>
<li><p><strong>eval_transform</strong> – The transformation to apply to the test data,
e.g. a random crop, a normalization or a concatenation of different
transformations (see torchvision.transform documentation for a
comprehensive list of possible transformations). Defaults to None.</p></li>
<li><p><strong>eval_target_transform</strong> – The transformation to apply to test
patterns targets. Defaults to None.</p></li>
<li><p><strong>dataset_type</strong> – The type of the dataset. Defaults to UNDEFINED.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A properly initialized <code class="xref py py-class docutils literal notranslate"><span class="pre">GenericCLScenario</span></code> instance.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="avalanche.benchmarks.generators.nc_benchmark">
<span class="sig-prename descclassname"><span class="pre">avalanche.benchmarks.generators.</span></span><span class="sig-name descname"><span class="pre">nc_benchmark</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">train_dataset</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">Sequence</span><span class="p"><span class="pre">[</span></span><span class="pre">SupportedDataset</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span> </span><span class="pre">SupportedDataset</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">test_dataset</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">Sequence</span><span class="p"><span class="pre">[</span></span><span class="pre">SupportedDataset</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span> </span><span class="pre">SupportedDataset</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_experiences</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">task_labels</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">bool</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">shuffle</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">bool</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">seed</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">fixed_class_order</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Sequence</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">per_exp_classes</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span> </span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">class_ids_from_zero_from_first_exp</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">bool</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">class_ids_from_zero_in_each_exp</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">bool</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">one_dataset_per_exp</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">bool</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">train_transform</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">eval_transform</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">reproducibility_data</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">NCScenario</span></span></span><a class="reference internal" href="../../../../_modules/avalanche/benchmarks/generators/benchmark_generators/#nc_benchmark"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.benchmarks.generators.nc_benchmark" title="Permalink to this definition"></a></dt>
<dd><p>This is the high-level benchmark instances generator for the
“New Classes” (NC) case. Given a sequence of train and test datasets creates
the continual stream of data as a series of experiences. Each experience
will contain all the instances belonging to a certain set of classes and a
class won’t be assigned to more than one experience.</p>
<p>This is the reference helper function for creating instances of Class- or
Task-Incremental benchmarks.</p>
<p>The <code class="docutils literal notranslate"><span class="pre">task_labels</span></code> parameter determines if each incremental experience has
an increasing task label or if, at the contrary, a default task label “0”
has to be assigned to all experiences. This can be useful when
differentiating between Single-Incremental-Task and Multi-Task scenarios.</p>
<p>There are other important parameters that can be specified in order to tweak
the behaviour of the resulting benchmark. Please take a few minutes to read
and understand them as they may save you a lot of work.</p>
<p>This generator features a integrated reproducibility mechanism that allows
the user to store and later re-load a benchmark. For more info see the
<code class="docutils literal notranslate"><span class="pre">reproducibility_data</span></code> parameter.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>train_dataset</strong> – A list of training datasets, or a single dataset.</p></li>
<li><p><strong>test_dataset</strong> – A list of test datasets, or a single test dataset.</p></li>
<li><p><strong>n_experiences</strong> – The number of incremental experience. This is not used
when using multiple train/test datasets with the <code class="docutils literal notranslate"><span class="pre">one_dataset_per_exp</span></code>
parameter set to True.</p></li>
<li><p><strong>task_labels</strong> – If True, each experience will have an ascending task
label. If False, the task label will be 0 for all the experiences.</p></li>
<li><p><strong>shuffle</strong> – If True, the class (or experience) order will be shuffled.
Defaults to True.</p></li>
<li><p><strong>seed</strong> – If <code class="docutils literal notranslate"><span class="pre">shuffle</span></code> is True and seed is not None, the class (or
experience) order will be shuffled according to the seed. When None, the
current PyTorch random number generator state will be used. Defaults to
None.</p></li>
<li><p><strong>fixed_class_order</strong> – If not None, the class order to use (overrides
the shuffle argument). Very useful for enhancing reproducibility.
Defaults to None.</p></li>
<li><p><strong>per_exp_classes</strong> – Is not None, a dictionary whose keys are
(0-indexed) experience IDs and their values are the number of classes
to include in the respective experiences. The dictionary doesn’t
have to contain a key for each experience! All the remaining experiences
will contain an equal amount of the remaining classes. The
remaining number of classes must be divisible without remainder
by the remaining number of experiences. For instance,
if you want to include 50 classes in the first experience
while equally distributing remaining classes across remaining
experiences, just pass the “{0: 50}” dictionary as the
per_experience_classes parameter. Defaults to None.</p></li>
<li><p><strong>class_ids_from_zero_from_first_exp</strong> – If True, original class IDs
will be remapped so that they will appear as having an ascending
order. For instance, if the resulting class order after shuffling
(or defined by fixed_class_order) is [23, 34, 11, 7, 6, …] and
class_ids_from_zero_from_first_exp is True, then all the patterns
belonging to class 23 will appear as belonging to class “0”,
class “34” will be mapped to “1”, class “11” to “2” and so on.
This is very useful when drawing confusion matrices and when dealing
with algorithms with dynamic head expansion. Defaults to False.
Mutually exclusive with the <code class="docutils literal notranslate"><span class="pre">class_ids_from_zero_in_each_exp</span></code>
parameter.</p></li>
<li><p><strong>class_ids_from_zero_in_each_exp</strong> – If True, original class IDs
will be mapped to range [0, n_classes_in_exp) for each experience.
Defaults to False. Mutually exclusive with the
<code class="docutils literal notranslate"><span class="pre">class_ids_from_zero_from_first_exp</span></code> parameter.</p></li>
<li><p><strong>one_dataset_per_exp</strong> – available only when multiple train-test
datasets are provided. If True, each dataset will be treated as a
experience. Mutually exclusive with the <code class="docutils literal notranslate"><span class="pre">per_experience_classes</span></code> and
<code class="docutils literal notranslate"><span class="pre">fixed_class_order</span></code> parameters. Overrides the <code class="docutils literal notranslate"><span class="pre">n_experiences</span></code>
parameter. Defaults to False.</p></li>
<li><p><strong>train_transform</strong> – The transformation to apply to the training data,
e.g. a random crop, a normalization or a concatenation of different
transformations (see torchvision.transform documentation for a
comprehensive list of possible transformations). Defaults to None.</p></li>
<li><p><strong>eval_transform</strong> – The transformation to apply to the test data,
e.g. a random crop, a normalization or a concatenation of different
transformations (see torchvision.transform documentation for a
comprehensive list of possible transformations). Defaults to None.</p></li>
<li><p><strong>reproducibility_data</strong> – If not None, overrides all the other
benchmark definition options. This is usually a dictionary containing
data used to reproduce a specific experiment. One can use the
<code class="docutils literal notranslate"><span class="pre">get_reproducibility_data</span></code> method to get (and even distribute)
the experiment setup so that it can be loaded by passing it as this
parameter. In this way one can be sure that the same specific
experimental setup is being used (for reproducibility purposes).
Beware that, in order to reproduce an experiment, the same train and
test datasets must be used. Defaults to None.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A properly initialized <code class="xref py py-class docutils literal notranslate"><span class="pre">NCScenario</span></code> instance.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="avalanche.benchmarks.generators.ni_benchmark">
<span class="sig-prename descclassname"><span class="pre">avalanche.benchmarks.generators.</span></span><span class="sig-name descname"><span class="pre">ni_benchmark</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">train_dataset</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">Sequence</span><span class="p"><span class="pre">[</span></span><span class="pre">SupportedDataset</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span> </span><span class="pre">SupportedDataset</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">test_dataset</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">Sequence</span><span class="p"><span class="pre">[</span></span><span class="pre">SupportedDataset</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span> </span><span class="pre">SupportedDataset</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_experiences</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">task_labels</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">bool</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">shuffle</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">bool</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">seed</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">balance_experiences</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">bool</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">min_class_patterns_in_exp</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">fixed_exp_assignment</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">Sequence</span><span class="p"><span class="pre">[</span></span><span class="pre">Sequence</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">train_transform</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">eval_transform</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">reproducibility_data</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">NIScenario</span></span></span><a class="reference internal" href="../../../../_modules/avalanche/benchmarks/generators/benchmark_generators/#ni_benchmark"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.benchmarks.generators.ni_benchmark" title="Permalink to this definition"></a></dt>
<dd><p>This is the high-level benchmark instances generator for the
“New Instances” (NI) case. Given a sequence of train and test datasets
creates the continual stream of data as a series of experiences.</p>
<p>This is the reference helper function for creating instances of
Domain-Incremental benchmarks.</p>
<p>The <code class="docutils literal notranslate"><span class="pre">task_labels</span></code> parameter determines if each incremental experience has
an increasing task label or if, at the contrary, a default task label “0”
has to be assigned to all experiences. This can be useful when
differentiating between Single-Incremental-Task and Multi-Task scenarios.</p>
<p>There are other important parameters that can be specified in order to tweak
the behaviour of the resulting benchmark. Please take a few minutes to read
and understand them as they may save you a lot of work.</p>
<p>This generator features an integrated reproducibility mechanism that allows
the user to store and later re-load a benchmark. For more info see the
<code class="docutils literal notranslate"><span class="pre">reproducibility_data</span></code> parameter.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>train_dataset</strong> – A list of training datasets, or a single dataset.</p></li>
<li><p><strong>test_dataset</strong> – A list of test datasets, or a single test dataset.</p></li>
<li><p><strong>n_experiences</strong> – The number of experiences.</p></li>
<li><p><strong>task_labels</strong> – If True, each experience will have an ascending task
label. If False, the task label will be 0 for all the experiences.</p></li>
<li><p><strong>shuffle</strong> – If True, patterns order will be shuffled.</p></li>
<li><p><strong>seed</strong> – A valid int used to initialize the random number generator.
Can be None.</p></li>
<li><p><strong>balance_experiences</strong> – If True, pattern of each class will be equally
spread across all experiences. If False, patterns will be assigned to
experiences in a complete random way. Defaults to False.</p></li>
<li><p><strong>min_class_patterns_in_exp</strong> – The minimum amount of patterns of
every class that must be assigned to every experience. Compatible with
the <code class="docutils literal notranslate"><span class="pre">balance_experiences</span></code> parameter. An exception will be raised if
this constraint can’t be satisfied. Defaults to 0.</p></li>
<li><p><strong>fixed_exp_assignment</strong> – If not None, the pattern assignment
to use. It must be a list with an entry for each experience. Each entry
is a list that contains the indexes of patterns belonging to that
experience. Overrides the <code class="docutils literal notranslate"><span class="pre">shuffle</span></code>, <code class="docutils literal notranslate"><span class="pre">balance_experiences</span></code> and
<code class="docutils literal notranslate"><span class="pre">min_class_patterns_in_exp</span></code> parameters.</p></li>
<li><p><strong>train_transform</strong> – The transformation to apply to the training data,
e.g. a random crop, a normalization or a concatenation of different
transformations (see torchvision.transform documentation for a
comprehensive list of possible transformations). Defaults to None.</p></li>
<li><p><strong>eval_transform</strong> – The transformation to apply to the test data,
e.g. a random crop, a normalization or a concatenation of different
transformations (see torchvision.transform documentation for a
comprehensive list of possible transformations). Defaults to None.</p></li>
<li><p><strong>reproducibility_data</strong> – If not None, overrides all the other
benchmark definition options, including <code class="docutils literal notranslate"><span class="pre">fixed_exp_assignment</span></code>.
This is usually a dictionary containing data used to
reproduce a specific experiment. One can use the
<code class="docutils literal notranslate"><span class="pre">get_reproducibility_data</span></code> method to get (and even distribute)
the experiment setup so that it can be loaded by passing it as this
parameter. In this way one can be sure that the same specific
experimental setup is being used (for reproducibility purposes).
Beware that, in order to reproduce an experiment, the same train and
test datasets must be used. Defaults to None.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A properly initialized <code class="xref py py-class docutils literal notranslate"><span class="pre">NIScenario</span></code> instance.</p>
</dd>
</dl>
</dd></dl>

<dl class="py data">
<dt class="sig sig-object py" id="avalanche.benchmarks.generators.dataset_benchmark">
<span class="sig-prename descclassname"><span class="pre">avalanche.benchmarks.generators.</span></span><span class="sig-name descname"><span class="pre">dataset_benchmark</span></span><a class="headerlink" href="#avalanche.benchmarks.generators.dataset_benchmark" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py data">
<dt class="sig sig-object py" id="avalanche.benchmarks.generators.filelist_benchmark">
<span class="sig-prename descclassname"><span class="pre">avalanche.benchmarks.generators.</span></span><span class="sig-name descname"><span class="pre">filelist_benchmark</span></span><a class="headerlink" href="#avalanche.benchmarks.generators.filelist_benchmark" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py data">
<dt class="sig sig-object py" id="avalanche.benchmarks.generators.paths_benchmark">
<span class="sig-prename descclassname"><span class="pre">avalanche.benchmarks.generators.</span></span><span class="sig-name descname"><span class="pre">paths_benchmark</span></span><a class="headerlink" href="#avalanche.benchmarks.generators.paths_benchmark" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py data">
<dt class="sig sig-object py" id="avalanche.benchmarks.generators.tensors_benchmark">
<span class="sig-prename descclassname"><span class="pre">avalanche.benchmarks.generators.</span></span><span class="sig-name descname"><span class="pre">tensors_benchmark</span></span><a class="headerlink" href="#avalanche.benchmarks.generators.tensors_benchmark" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="avalanche.benchmarks.generators.data_incremental_benchmark">
<span class="sig-prename descclassname"><span class="pre">avalanche.benchmarks.generators.</span></span><span class="sig-name descname"><span class="pre">data_incremental_benchmark</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">benchmark_instance</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">GenericCLScenario</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">experience_size</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">shuffle</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">bool</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">drop_last</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">bool</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">split_streams</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Sequence</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">('train',)</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">custom_split_strategy</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Callable</span><span class="p"><span class="pre">[</span></span><span class="p"><span class="pre">[</span></span><span class="pre">Experience</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span> </span><span class="pre">Sequence</span><span class="p"><span class="pre">[</span></span><span class="pre">AvalancheDataset</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">experience_factory</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Callable</span><span class="p"><span class="pre">[</span></span><span class="p"><span class="pre">[</span></span><span class="pre">GenericScenarioStream</span><span class="p"><span class="pre">,</span> </span><span class="pre">int</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span> </span><span class="pre">Experience</span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../../_modules/avalanche/benchmarks/generators/benchmark_generators/#data_incremental_benchmark"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.benchmarks.generators.data_incremental_benchmark" title="Permalink to this definition"></a></dt>
<dd><p>High-level benchmark generator for a Data Incremental setup.</p>
<p>This generator accepts an existing benchmark instance and returns a version
of it in which experiences have been split in order to produce a
Data Incremental stream.</p>
<p>In its base form this generator will split train experiences in experiences
of a fixed, configurable, size. The split can be also performed on other
streams (like the test one) if needed.</p>
<p>The <cite>custom_split_strategy</cite> parameter can be used if a more specific
splitting is required.</p>
<p>Beware that experience splitting is NOT executed in a lazy way. This
means that the splitting process takes place immediately. Consider
optimizing the split process for speed when using a custom splitting
strategy.</p>
<p>Please note that each mini-experience will have a task labels field
equal to the one of the originating experience.</p>
<p>The <cite>complete_test_set_only</cite> field of the resulting benchmark instance
will be <cite>True</cite> only if the same field of original benchmark instance is
<cite>True</cite> and if the resulting test stream contains exactly one experience.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>benchmark_instance</strong> – The benchmark to split.</p></li>
<li><p><strong>experience_size</strong> – The size of the experience, as an int. Ignored
if <cite>custom_split_strategy</cite> is used.</p></li>
<li><p><strong>shuffle</strong> – If True, experiences will be split by first shuffling
instances in each experience. This will use the default PyTorch
random number generator at its current state. Defaults to False.
Ignored if <cite>custom_split_strategy</cite> is used.</p></li>
<li><p><strong>drop_last</strong> – If True, if the last experience doesn’t contain
<cite>experience_size</cite> instances, then the last experience will be dropped.
Defaults to False. Ignored if <cite>custom_split_strategy</cite> is used.</p></li>
<li><p><strong>split_streams</strong> – The list of streams to split. By default only the
“train” stream will be split.</p></li>
<li><p><strong>custom_split_strategy</strong> – A function that implements a custom splitting
strategy. The function must accept an experience and return a list
of datasets each describing an experience. Defaults to None, which means
that the standard splitting strategy will be used (which creates
experiences of size <cite>experience_size</cite>).
A good starting to understand the mechanism is to look at the
implementation of the standard splitting function
<code class="xref py py-func docutils literal notranslate"><span class="pre">fixed_size_experience_split_strategy()</span></code>.</p></li>
<li><p><strong>experience_factory</strong> – The experience factory.
Defaults to <code class="xref py py-class docutils literal notranslate"><span class="pre">GenericExperience</span></code>.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The Data Incremental benchmark instance.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="avalanche.benchmarks.generators.benchmark_with_validation_stream">
<span class="sig-prename descclassname"><span class="pre">avalanche.benchmarks.generators.</span></span><span class="sig-name descname"><span class="pre">benchmark_with_validation_stream</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">benchmark_instance</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">GenericCLScenario</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">validation_size</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span> </span><span class="pre">float</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">shuffle</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">bool</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">input_stream</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">str</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">'train'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_stream</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">str</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">'valid'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">custom_split_strategy</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Callable</span><span class="p"><span class="pre">[</span></span><span class="p"><span class="pre">[</span></span><span class="pre">Experience</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span> </span><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">AvalancheDataset</span><span class="p"><span class="pre">,</span> </span><span class="pre">AvalancheDataset</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">experience_factory</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Callable</span><span class="p"><span class="pre">[</span></span><span class="p"><span class="pre">[</span></span><span class="pre">GenericScenarioStream</span><span class="p"><span class="pre">,</span> </span><span class="pre">int</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span> </span><span class="pre">Experience</span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">lazy_splitting</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">bool</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../../_modules/avalanche/benchmarks/generators/benchmark_generators/#benchmark_with_validation_stream"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.benchmarks.generators.benchmark_with_validation_stream" title="Permalink to this definition"></a></dt>
<dd><p>Helper that can be used to obtain a benchmark with a validation stream.</p>
<p>This generator accepts an existing benchmark instance and returns a version
of it in which a validation stream has been added.</p>
<p>In its base form this generator will split train experiences to extract
validation experiences of a fixed (by number of instances or relative
size), configurable, size. The split can be also performed on other
streams if needed and the name of the resulting validation stream can
be configured too.</p>
<p>Each validation experience will be extracted directly from a single training
experience. Patterns selected for the validation experience will be removed
from the training one.</p>
<p>If shuffle is True, the validation stream will be created randomly.
Beware that no kind of class balancing is done.</p>
<p>The <cite>custom_split_strategy</cite> parameter can be used if a more specific
splitting is required.</p>
<p>Please note that the resulting experiences will have a task labels field
equal to the one of the originating experience.</p>
<p>Experience splitting can be executed in a lazy way. This behavior can be
controlled using the <cite>lazy_splitting</cite> parameter. By default, experiences
are split in a lazy way only when the input stream is lazily generated.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>benchmark_instance</strong> – The benchmark to split.</p></li>
<li><p><strong>validation_size</strong> – The size of the validation experience, as an int
or a float between 0 and 1. Ignored if <cite>custom_split_strategy</cite> is used.</p></li>
<li><p><strong>shuffle</strong> – If True, patterns will be allocated to the validation
stream randomly. This will use the default PyTorch random number
generator at its current state. Defaults to False. Ignored if
<cite>custom_split_strategy</cite> is used. If False, the first instances will be
allocated to the training  dataset by leaving the last ones to the
validation dataset.</p></li>
<li><p><strong>input_stream</strong> – The name of the input stream. Defaults to ‘train’.</p></li>
<li><p><strong>output_stream</strong> – The name of the output stream. Defaults to ‘valid’.</p></li>
<li><p><strong>custom_split_strategy</strong> – A function that implements a custom splitting
strategy. The function must accept an experience and return a tuple
containing the new train and validation dataset. Defaults to None,
which means that the standard splitting strategy will be used (which
creates experiences according to <cite>validation_size</cite> and <cite>shuffle</cite>).
A good starting to understand the mechanism is to look at the
implementation of the standard splitting function
<code class="xref py py-func docutils literal notranslate"><span class="pre">random_validation_split_strategy()</span></code>.</p></li>
<li><p><strong>experience_factory</strong> – The experience factory. Defaults to
<code class="xref py py-class docutils literal notranslate"><span class="pre">GenericExperience</span></code>.</p></li>
<li><p><strong>lazy_splitting</strong> – If True, the stream will be split in a lazy way.
If False, the stream will be split immediately. Defaults to None, which
means that the stream will be split in a lazy or non-lazy way depending
on the laziness of the <cite>input_stream</cite>.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A benchmark instance in which the validation stream has been added.</p>
</dd>
</dl>
</dd></dl>

</section>
</section>
</section>


           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2021, ContinualAI.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>