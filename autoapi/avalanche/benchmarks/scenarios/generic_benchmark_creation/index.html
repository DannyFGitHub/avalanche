<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>avalanche.benchmarks.scenarios.generic_benchmark_creation &mdash; Avalanche 0.1 documentation</title>
      <link rel="stylesheet" href="../../../../../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../../../../../_static/css/mystyle.css" type="text/css" />
      <link rel="stylesheet" href="../../../../../_static/graphviz.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../../../../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="../../../../../" id="documentation_options" src="../../../../../_static/documentation_options.js"></script>
        <script src="../../../../../_static/jquery.js"></script>
        <script src="../../../../../_static/underscore.js"></script>
        <script src="../../../../../_static/doctools.js"></script>
    <script src="../../../../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../../../../genindex/" />
    <link rel="search" title="Search" href="../../../../../search/" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../../../../../">
            <img src="../../../../../_static/avalanche_logo.png" class="logo" alt="Logo"/>
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../../../search/" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Avalanche API:</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="reference internal" href="../../../../">API Reference</a><ul class="current">
<li class="toctree-l2 current"><a class="reference internal" href="../../../"><code class="xref py py-mod docutils literal notranslate"><span class="pre">avalanche</span></code></a><ul class="current">
<li class="toctree-l3 current"><a class="reference internal" href="../../../#subpackages">Subpackages</a><ul class="current">
<li class="toctree-l4 current"><a class="reference internal" href="../../"><code class="xref py py-mod docutils literal notranslate"><span class="pre">avalanche.benchmarks</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../evaluation/"><code class="xref py py-mod docutils literal notranslate"><span class="pre">avalanche.evaluation</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../logging/"><code class="xref py py-mod docutils literal notranslate"><span class="pre">avalanche.logging</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../models/"><code class="xref py py-mod docutils literal notranslate"><span class="pre">avalanche.models</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../training/"><code class="xref py py-mod docutils literal notranslate"><span class="pre">avalanche.training</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../#submodules">Submodules</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../core/"><code class="xref py py-mod docutils literal notranslate"><span class="pre">avalanche.core</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../#package-contents">Package Contents</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../#avalanche.__version__">__version__</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../../../">Avalanche</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../../../../" class="icon icon-home"></a> &raquo;</li>
      <li><code class="xref py py-mod docutils literal notranslate"><span class="pre">avalanche.benchmarks.scenarios.generic_benchmark_creation</span></code></li>
      <li class="wy-breadcrumbs-aside">
            <a href="../../../../../_sources/autoapi/avalanche/benchmarks/scenarios/generic_benchmark_creation/index.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="module-avalanche.benchmarks.scenarios.generic_benchmark_creation">
<span id="avalanche-benchmarks-scenarios-generic-benchmark-creation"></span><h1><a class="reference internal" href="#module-avalanche.benchmarks.scenarios.generic_benchmark_creation" title="avalanche.benchmarks.scenarios.generic_benchmark_creation"><code class="xref py py-mod docutils literal notranslate"><span class="pre">avalanche.benchmarks.scenarios.generic_benchmark_creation</span></code></a><a class="headerlink" href="#module-avalanche.benchmarks.scenarios.generic_benchmark_creation" title="Permalink to this headline"></a></h1>
<p>This module contains mid-level benchmark generators.
Consider using the higher-level ones found in benchmark_generators. If none of
them fit your needs, then the helper functions here listed may help.</p>
<section id="module-contents">
<h2>Module Contents<a class="headerlink" href="#module-contents" title="Permalink to this headline"></a></h2>
<section id="classes">
<h3>Classes<a class="headerlink" href="#classes" title="Permalink to this headline"></a></h3>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#avalanche.benchmarks.scenarios.generic_benchmark_creation.LazyStreamDefinition" title="avalanche.benchmarks.scenarios.generic_benchmark_creation.LazyStreamDefinition"><code class="xref py py-obj docutils literal notranslate"><span class="pre">LazyStreamDefinition</span></code></a></p></td>
<td><p>A simple class that can be used when preparing the parameters for the</p></td>
</tr>
</tbody>
</table>
</section>
<section id="functions">
<h3>Functions<a class="headerlink" href="#functions" title="Permalink to this headline"></a></h3>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#avalanche.benchmarks.scenarios.generic_benchmark_creation.create_multi_dataset_generic_benchmark" title="avalanche.benchmarks.scenarios.generic_benchmark_creation.create_multi_dataset_generic_benchmark"><code class="xref py py-obj docutils literal notranslate"><span class="pre">create_multi_dataset_generic_benchmark</span></code></a>(train_datasets: Sequence[SupportedDataset], test_datasets: Sequence[SupportedDataset], *, other_streams_datasets: Dict[str, Sequence[SupportedDataset]] = None, complete_test_set_only: bool = False, train_transform=None, train_target_transform=None, eval_transform=None, eval_target_transform=None, other_streams_transforms: Dict[str, Tuple[Any, Any]] = None, dataset_type: AvalancheDatasetType = None) → GenericCLScenario</p></td>
<td><p>Creates a benchmark instance given a list of datasets. Each dataset will be</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#avalanche.benchmarks.scenarios.generic_benchmark_creation.create_lazy_generic_benchmark" title="avalanche.benchmarks.scenarios.generic_benchmark_creation.create_lazy_generic_benchmark"><code class="xref py py-obj docutils literal notranslate"><span class="pre">create_lazy_generic_benchmark</span></code></a>(train_generator: LazyStreamDefinition, test_generator: LazyStreamDefinition, *, other_streams_generators: Dict[str, LazyStreamDefinition] = None, complete_test_set_only: bool = False, train_transform=None, train_target_transform=None, eval_transform=None, eval_target_transform=None, other_streams_transforms: Dict[str, Tuple[Any, Any]] = None, dataset_type: AvalancheDatasetType = None) → GenericCLScenario</p></td>
<td><p>Creates a lazily-defined benchmark instance given a dataset generator for</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#avalanche.benchmarks.scenarios.generic_benchmark_creation.create_generic_benchmark_from_filelists" title="avalanche.benchmarks.scenarios.generic_benchmark_creation.create_generic_benchmark_from_filelists"><code class="xref py py-obj docutils literal notranslate"><span class="pre">create_generic_benchmark_from_filelists</span></code></a>(root: Optional[Union[str, Path]], train_file_lists: Sequence[Union[str, Path]], test_file_lists: Sequence[Union[str, Path]], *, other_streams_file_lists: Dict[str, Sequence[Union[str, Path]]] = None, task_labels: Sequence[int], complete_test_set_only: bool = False, train_transform=None, train_target_transform=None, eval_transform=None, eval_target_transform=None, other_streams_transforms: Dict[str, Tuple[Any, Any]] = None) → GenericCLScenario</p></td>
<td><p>Creates a benchmark instance given a list of filelists and the respective</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#avalanche.benchmarks.scenarios.generic_benchmark_creation.create_generic_benchmark_from_paths" title="avalanche.benchmarks.scenarios.generic_benchmark_creation.create_generic_benchmark_from_paths"><code class="xref py py-obj docutils literal notranslate"><span class="pre">create_generic_benchmark_from_paths</span></code></a>(train_lists_of_files: Sequence[Sequence[FileAndLabel]], test_lists_of_files: Union[Sequence[FileAndLabel], Sequence[Sequence[FileAndLabel]]], *, other_streams_lists_of_files: Dict[str, Sequence[Sequence[FileAndLabel]]] = None, task_labels: Sequence[int], complete_test_set_only: bool = False, train_transform=None, train_target_transform=None, eval_transform=None, eval_target_transform=None, other_streams_transforms: Dict[str, Tuple[Any, Any]] = None, dataset_type: AvalancheDatasetType = AvalancheDatasetType.UNDEFINED) → GenericCLScenario</p></td>
<td><p>Creates a benchmark instance given a sequence of lists of files. A separate</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#avalanche.benchmarks.scenarios.generic_benchmark_creation.create_generic_benchmark_from_tensor_lists" title="avalanche.benchmarks.scenarios.generic_benchmark_creation.create_generic_benchmark_from_tensor_lists"><code class="xref py py-obj docutils literal notranslate"><span class="pre">create_generic_benchmark_from_tensor_lists</span></code></a>(train_tensors: Sequence[Sequence[Any]], test_tensors: Sequence[Sequence[Any]], *, other_streams_tensors: Dict[str, Sequence[Sequence[Any]]] = None, task_labels: Sequence[int], complete_test_set_only: bool = False, train_transform=None, train_target_transform=None, eval_transform=None, eval_target_transform=None, other_streams_transforms: Dict[str, Tuple[Any, Any]] = None, dataset_type: AvalancheDatasetType = None) → GenericCLScenario</p></td>
<td><p>Creates a benchmark instance given lists of Tensors. A separate dataset will</p></td>
</tr>
</tbody>
</table>
<dl class="py function">
<dt class="sig sig-object py" id="avalanche.benchmarks.scenarios.generic_benchmark_creation.create_multi_dataset_generic_benchmark">
<span class="sig-prename descclassname"><span class="pre">avalanche.benchmarks.scenarios.generic_benchmark_creation.</span></span><span class="sig-name descname"><span class="pre">create_multi_dataset_generic_benchmark</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">train_datasets</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Sequence</span><span class="p"><span class="pre">[</span></span><span class="pre">SupportedDataset</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">test_datasets</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Sequence</span><span class="p"><span class="pre">[</span></span><span class="pre">SupportedDataset</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">other_streams_datasets</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span> </span><span class="pre">Sequence</span><span class="p"><span class="pre">[</span></span><span class="pre">SupportedDataset</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">complete_test_set_only</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">bool</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">train_transform</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">train_target_transform</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">eval_transform</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">eval_target_transform</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">other_streams_transforms</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span> </span><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">Any</span><span class="p"><span class="pre">,</span> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dataset_type</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">AvalancheDatasetType</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">GenericCLScenario</span></span></span><a class="reference internal" href="../../../../../_modules/avalanche/benchmarks/scenarios/generic_benchmark_creation/#create_multi_dataset_generic_benchmark"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.benchmarks.scenarios.generic_benchmark_creation.create_multi_dataset_generic_benchmark" title="Permalink to this definition"></a></dt>
<dd><p>Creates a benchmark instance given a list of datasets. Each dataset will be
considered as a separate experience.</p>
<p>Contents of the datasets must already be set, including task labels.
Transformations will be applied if defined.</p>
<p>This function allows for the creation of custom streams as well.
While “train” and “test” datasets must always be set, the experience list
for other streams can be defined by using the <cite>other_streams_datasets</cite>
parameter.</p>
<p>If transformations are defined, they will be applied to the datasets
of the related stream.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>train_datasets</strong> – A list of training datasets.</p></li>
<li><p><strong>test_datasets</strong> – A list of test datasets.</p></li>
<li><p><strong>other_streams_datasets</strong> – A dictionary describing the content of custom
streams. Keys must be valid stream names (letters and numbers,
not starting with a number) while the value must be a list of dataset.
If this dictionary contains the definition for “train” or “test”
streams then those definition will override the <cite>train_datasets</cite> and
<cite>test_datasets</cite> parameters.</p></li>
<li><p><strong>complete_test_set_only</strong> – If True, only the complete test set will
be returned by the benchmark. This means that the <code class="docutils literal notranslate"><span class="pre">test_dataset_list</span></code>
parameter must be list with a single element (the complete test set).
Defaults to False.</p></li>
<li><p><strong>train_transform</strong> – The transformation to apply to the training data,
e.g. a random crop, a normalization or a concatenation of different
transformations (see torchvision.transform documentation for a
comprehensive list of possible transformations). Defaults to None.</p></li>
<li><p><strong>train_target_transform</strong> – The transformation to apply to training
patterns targets. Defaults to None.</p></li>
<li><p><strong>eval_transform</strong> – The transformation to apply to the test data,
e.g. a random crop, a normalization or a concatenation of different
transformations (see torchvision.transform documentation for a
comprehensive list of possible transformations). Defaults to None.</p></li>
<li><p><strong>eval_target_transform</strong> – The transformation to apply to test
patterns targets. Defaults to None.</p></li>
<li><p><strong>other_streams_transforms</strong> – Transformations to apply to custom
streams. If no transformations are defined for a custom stream,
then “train” transformations will be used. This parameter must be a
dictionary mapping stream names to transformations. The transformations
must be a two elements tuple where the first element defines the
X transformation while the second element is the Y transformation.
Those elements can be None. If this dictionary contains the
transformations for “train” or “test” streams then those transformations
will override the <cite>train_transform</cite>, <cite>train_target_transform</cite>,
<cite>eval_transform</cite> and <cite>eval_target_transform</cite> parameters.</p></li>
<li><p><strong>dataset_type</strong> – The type of the dataset. Defaults to None, which
means that the type will be obtained from the input datasets. If input
datasets are not instances of <code class="xref py py-class docutils literal notranslate"><span class="pre">AvalancheDataset</span></code>, the type
UNDEFINED will be used.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A <code class="xref py py-class docutils literal notranslate"><span class="pre">GenericCLScenario</span></code> instance.</p>
</dd>
</dl>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="avalanche.benchmarks.scenarios.generic_benchmark_creation.LazyStreamDefinition">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">avalanche.benchmarks.scenarios.generic_benchmark_creation.</span></span><span class="sig-name descname"><span class="pre">LazyStreamDefinition</span></span><a class="reference internal" href="../../../../../_modules/avalanche/benchmarks/scenarios/generic_benchmark_creation/#LazyStreamDefinition"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.benchmarks.scenarios.generic_benchmark_creation.LazyStreamDefinition" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">typing.NamedTuple</span></code></p>
<p>A simple class that can be used when preparing the parameters for the
<a class="reference internal" href="#avalanche.benchmarks.scenarios.generic_benchmark_creation.create_lazy_generic_benchmark" title="avalanche.benchmarks.scenarios.generic_benchmark_creation.create_lazy_generic_benchmark"><code class="xref py py-func docutils literal notranslate"><span class="pre">create_lazy_generic_benchmark()</span></code></a> helper.</p>
<p>This class is a named tuple containing the fields required for defining
a lazily-created benchmark.</p>
<ul class="simple">
<li><p>exps_generator: The experiences generator. Can be a “yield”-based
generator, a custom sequence, a standard list or any kind of
iterable returning <code class="xref py py-class docutils literal notranslate"><span class="pre">AvalancheDataset</span></code>.</p></li>
<li><p>stream_length: The number of experiences in the stream. Must match the
number of experiences returned by the generator.</p></li>
<li><p>exps_task_labels: A list containing the list of task labels of each
experience. If an experience contains a single task label, a single int
can be used.</p></li>
</ul>
<p>Create and return a new object.  See help(type) for accurate signature.</p>
<dl class="py attribute">
<dt class="sig sig-object py" id="avalanche.benchmarks.scenarios.generic_benchmark_creation.LazyStreamDefinition.exps_generator">
<span class="sig-name descname"><span class="pre">exps_generator</span></span><em class="property"> <span class="pre">:Iterable[AvalancheDataset]</span></em><a class="headerlink" href="#avalanche.benchmarks.scenarios.generic_benchmark_creation.LazyStreamDefinition.exps_generator" title="Permalink to this definition"></a></dt>
<dd><p>The experiences generator. Can be a “yield”-based generator, a custom
sequence, a standard list or any kind of iterable returning
<code class="xref py py-class docutils literal notranslate"><span class="pre">AvalancheDataset</span></code>.</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="avalanche.benchmarks.scenarios.generic_benchmark_creation.LazyStreamDefinition.stream_length">
<span class="sig-name descname"><span class="pre">stream_length</span></span><em class="property"> <span class="pre">:int</span></em><a class="headerlink" href="#avalanche.benchmarks.scenarios.generic_benchmark_creation.LazyStreamDefinition.stream_length" title="Permalink to this definition"></a></dt>
<dd><p>The number of experiences in the stream. Must match the number of
experiences returned by the generator</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="avalanche.benchmarks.scenarios.generic_benchmark_creation.LazyStreamDefinition.exps_task_labels">
<span class="sig-name descname"><span class="pre">exps_task_labels</span></span><em class="property"> <span class="pre">:Sequence[Union[int,</span> <span class="pre">Iterable[int]]]</span></em><a class="headerlink" href="#avalanche.benchmarks.scenarios.generic_benchmark_creation.LazyStreamDefinition.exps_task_labels" title="Permalink to this definition"></a></dt>
<dd><p>A list containing the list of task labels of each experience.
If an experience contains a single task label, a single int can be used.</p>
<p>This field is temporary required for internal purposes to support lazy
streams. This field may become optional in the future.</p>
</dd></dl>

</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="avalanche.benchmarks.scenarios.generic_benchmark_creation.create_lazy_generic_benchmark">
<span class="sig-prename descclassname"><span class="pre">avalanche.benchmarks.scenarios.generic_benchmark_creation.</span></span><span class="sig-name descname"><span class="pre">create_lazy_generic_benchmark</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">train_generator</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><a class="reference internal" href="#avalanche.benchmarks.scenarios.generic_benchmark_creation.LazyStreamDefinition" title="avalanche.benchmarks.scenarios.generic_benchmark_creation.LazyStreamDefinition"><span class="pre">LazyStreamDefinition</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">test_generator</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><a class="reference internal" href="#avalanche.benchmarks.scenarios.generic_benchmark_creation.LazyStreamDefinition" title="avalanche.benchmarks.scenarios.generic_benchmark_creation.LazyStreamDefinition"><span class="pre">LazyStreamDefinition</span></a></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">other_streams_generators</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span> </span><a class="reference internal" href="#avalanche.benchmarks.scenarios.generic_benchmark_creation.LazyStreamDefinition" title="avalanche.benchmarks.scenarios.generic_benchmark_creation.LazyStreamDefinition"><span class="pre">LazyStreamDefinition</span></a><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">complete_test_set_only</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">bool</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">train_transform</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">train_target_transform</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">eval_transform</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">eval_target_transform</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">other_streams_transforms</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span> </span><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">Any</span><span class="p"><span class="pre">,</span> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dataset_type</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">AvalancheDatasetType</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">GenericCLScenario</span></span></span><a class="reference internal" href="../../../../../_modules/avalanche/benchmarks/scenarios/generic_benchmark_creation/#create_lazy_generic_benchmark"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.benchmarks.scenarios.generic_benchmark_creation.create_lazy_generic_benchmark" title="Permalink to this definition"></a></dt>
<dd><p>Creates a lazily-defined benchmark instance given a dataset generator for
each stream.</p>
<p>Generators must return properly initialized instances of
<code class="xref py py-class docutils literal notranslate"><span class="pre">AvalancheDataset</span></code> which will be used to create experiences.</p>
<p>The created datasets can have transformations already set.
However, if transformations are shared across all datasets of the same
stream, it is recommended to use the <cite>train_transform</cite>, <cite>eval_transform</cite>
and <cite>other_streams_transforms</cite> parameters, so that transformations groups
can be correctly applied (transformations are lazily added atop the datasets
returned by the generators). The same reasoning applies to the
<cite>dataset_type</cite> parameter.</p>
<p>This function allows for the creation of custom streams as well.
While “train” and “test” streams must be always set, the generators
for other streams can be defined by using the <cite>other_streams_generators</cite>
parameter.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>train_generator</strong> – A proper lazy-generation definition for the training
stream. It is recommended to pass an instance
of <a class="reference internal" href="#avalanche.benchmarks.scenarios.generic_benchmark_creation.LazyStreamDefinition" title="avalanche.benchmarks.scenarios.generic_benchmark_creation.LazyStreamDefinition"><code class="xref py py-class docutils literal notranslate"><span class="pre">LazyStreamDefinition</span></code></a>. See its description for more details.</p></li>
<li><p><strong>test_generator</strong> – A proper lazy-generation definition for the test
stream. It is recommended to pass an instance
of <a class="reference internal" href="#avalanche.benchmarks.scenarios.generic_benchmark_creation.LazyStreamDefinition" title="avalanche.benchmarks.scenarios.generic_benchmark_creation.LazyStreamDefinition"><code class="xref py py-class docutils literal notranslate"><span class="pre">LazyStreamDefinition</span></code></a>. See its description for more details.</p></li>
<li><p><strong>other_streams_generators</strong> – A dictionary describing the content of
custom streams. Keys must be valid stream names (letters and numbers,
not starting with a number) while the value must be a
lazy-generation definition (like the ones of the training and
test streams). If this dictionary contains the definition for
“train” or “test” streams then those definition will override the
<cite>train_generator</cite> and <cite>test_generator</cite> parameters.</p></li>
<li><p><strong>complete_test_set_only</strong> – If True, only the complete test set will
be returned by the benchmark. This means that the <code class="docutils literal notranslate"><span class="pre">test_generator</span></code>
parameter must define a stream with a single experience (the complete
test set). Defaults to False.</p></li>
<li><p><strong>train_transform</strong> – The transformation to apply to the training data,
e.g. a random crop, a normalization or a concatenation of different
transformations (see torchvision.transform documentation for a
comprehensive list of possible transformations). Defaults to None.</p></li>
<li><p><strong>train_target_transform</strong> – The transformation to apply to training
patterns targets. Defaults to None.</p></li>
<li><p><strong>eval_transform</strong> – The transformation to apply to the test data,
e.g. a random crop, a normalization or a concatenation of different
transformations (see torchvision.transform documentation for a
comprehensive list of possible transformations). Defaults to None.</p></li>
<li><p><strong>eval_target_transform</strong> – The transformation to apply to test
patterns targets. Defaults to None.</p></li>
<li><p><strong>other_streams_transforms</strong> – Transformations to apply to custom
streams. If no transformations are defined for a custom stream,
then “train” transformations will be used. This parameter must be a
dictionary mapping stream names to transformations. The transformations
must be a two elements tuple where the first element defines the
X transformation while the second element is the Y transformation.
Those elements can be None. If this dictionary contains the
transformations for “train” or “test” streams then those transformations
will override the <cite>train_transform</cite>, <cite>train_target_transform</cite>,
<cite>eval_transform</cite> and <cite>eval_target_transform</cite> parameters.</p></li>
<li><p><strong>dataset_type</strong> – The type of the datasets. Defaults to None, which
means that the type will be obtained from the input datasets. This
type will be applied to all the datasets returned by the generators.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A lazily-initialized <code class="xref py py-class docutils literal notranslate"><span class="pre">GenericCLScenario</span></code> instance.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="avalanche.benchmarks.scenarios.generic_benchmark_creation.create_generic_benchmark_from_filelists">
<span class="sig-prename descclassname"><span class="pre">avalanche.benchmarks.scenarios.generic_benchmark_creation.</span></span><span class="sig-name descname"><span class="pre">create_generic_benchmark_from_filelists</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">root</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span> </span><span class="pre">Path</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">train_file_lists</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Sequence</span><span class="p"><span class="pre">[</span></span><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span> </span><span class="pre">Path</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">test_file_lists</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Sequence</span><span class="p"><span class="pre">[</span></span><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span> </span><span class="pre">Path</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">other_streams_file_lists</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span> </span><span class="pre">Sequence</span><span class="p"><span class="pre">[</span></span><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span> </span><span class="pre">Path</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">task_labels</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Sequence</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">complete_test_set_only</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">bool</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">train_transform</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">train_target_transform</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">eval_transform</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">eval_target_transform</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">other_streams_transforms</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span> </span><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">Any</span><span class="p"><span class="pre">,</span> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">GenericCLScenario</span></span></span><a class="reference internal" href="../../../../../_modules/avalanche/benchmarks/scenarios/generic_benchmark_creation/#create_generic_benchmark_from_filelists"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.benchmarks.scenarios.generic_benchmark_creation.create_generic_benchmark_from_filelists" title="Permalink to this definition"></a></dt>
<dd><p>Creates a benchmark instance given a list of filelists and the respective
task labels. A separate dataset will be created for each filelist and each
of those datasets will be considered a separate experience.</p>
<p>This helper functions is the best shot when loading Caffe-style dataset
based on filelists.</p>
<p>Beware that this helper function is limited is the following two aspects:</p>
<ul class="simple">
<li><p>The resulting benchmark instance and the intermediate datasets used to
populate it will be of type CLASSIFICATION. There is no way to change
this.</p></li>
<li><p>Task labels can only be defined by choosing a single task label for
each experience (the same task label is applied to all patterns of
experiences sharing the same position in different streams).</p></li>
</ul>
<p>Despite those constraints, this helper function is usually sufficiently
powerful to cover most continual learning benchmarks based on file lists.</p>
<p>When in need to create a similar benchmark instance starting from an
in-memory list of paths, then the similar helper function
<a class="reference internal" href="#avalanche.benchmarks.scenarios.generic_benchmark_creation.create_generic_benchmark_from_paths" title="avalanche.benchmarks.scenarios.generic_benchmark_creation.create_generic_benchmark_from_paths"><code class="xref py py-func docutils literal notranslate"><span class="pre">create_generic_benchmark_from_paths()</span></code></a> can be used.</p>
<p>When in need to create a benchmark instance in which task labels are defined
in a more fine-grained way, then consider using
<a class="reference internal" href="#avalanche.benchmarks.scenarios.generic_benchmark_creation.create_multi_dataset_generic_benchmark" title="avalanche.benchmarks.scenarios.generic_benchmark_creation.create_multi_dataset_generic_benchmark"><code class="xref py py-func docutils literal notranslate"><span class="pre">create_multi_dataset_generic_benchmark()</span></code></a> by passing properly
initialized <code class="xref py py-class docutils literal notranslate"><span class="pre">AvalancheDataset</span></code> instances.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>root</strong> – The root path of the dataset. Can be None.</p></li>
<li><p><strong>train_file_lists</strong> – A list of filelists describing the
paths of the training patterns for each experience.</p></li>
<li><p><strong>test_file_lists</strong> – A list of filelists describing the
paths of the test patterns for each experience.</p></li>
<li><p><strong>other_streams_file_lists</strong> – A dictionary describing the content of
custom streams. Keys must be valid stream names (letters and numbers,
not starting with a number) while the value must be a list of filelists
(same as <cite>train_file_lists</cite> and <cite>test_file_lists</cite> parameters). If this
dictionary contains the definition for “train” or “test” streams then
those definition will  override the <cite>train_file_lists</cite> and
<cite>test_file_lists</cite> parameters.</p></li>
<li><p><strong>task_labels</strong> – A list of task labels. Must contain at least a value
for each experience. Each value describes the task label that will be
applied to all patterns of a certain experience. For more info on that,
see the function description.</p></li>
<li><p><strong>complete_test_set_only</strong> – If True, only the complete test set will
be returned by the benchmark. This means that the <code class="docutils literal notranslate"><span class="pre">test_file_lists</span></code>
parameter must be list with a single element (the complete test set).
Alternatively, can be a plain string or <code class="xref py py-class docutils literal notranslate"><span class="pre">Path</span></code> object.
Defaults to False.</p></li>
<li><p><strong>train_transform</strong> – The transformation to apply to the training data,
e.g. a random crop, a normalization or a concatenation of different
transformations (see torchvision.transform documentation for a
comprehensive list of possible transformations). Defaults to None.</p></li>
<li><p><strong>train_target_transform</strong> – The transformation to apply to training
patterns targets. Defaults to None.</p></li>
<li><p><strong>eval_transform</strong> – The transformation to apply to the test data,
e.g. a random crop, a normalization or a concatenation of different
transformations (see torchvision.transform documentation for a
comprehensive list of possible transformations). Defaults to None.</p></li>
<li><p><strong>eval_target_transform</strong> – The transformation to apply to test
patterns targets. Defaults to None.</p></li>
<li><p><strong>other_streams_transforms</strong> – Transformations to apply to custom
streams. If no transformations are defined for a custom stream,
then “train” transformations will be used. This parameter must be a
dictionary mapping stream names to transformations. The transformations
must be a two elements tuple where the first element defines the
X transformation while the second element is the Y transformation.
Those elements can be None. If this dictionary contains the
transformations for “train” or “test” streams then those transformations
will override the <cite>train_transform</cite>, <cite>train_target_transform</cite>,
<cite>eval_transform</cite> and <cite>eval_target_transform</cite> parameters.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A <code class="xref py py-class docutils literal notranslate"><span class="pre">GenericCLScenario</span></code> instance.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="avalanche.benchmarks.scenarios.generic_benchmark_creation.create_generic_benchmark_from_paths">
<span class="sig-prename descclassname"><span class="pre">avalanche.benchmarks.scenarios.generic_benchmark_creation.</span></span><span class="sig-name descname"><span class="pre">create_generic_benchmark_from_paths</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">train_lists_of_files</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Sequence</span><span class="p"><span class="pre">[</span></span><span class="pre">Sequence</span><span class="p"><span class="pre">[</span></span><span class="pre">FileAndLabel</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">test_lists_of_files</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">Sequence</span><span class="p"><span class="pre">[</span></span><span class="pre">FileAndLabel</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span> </span><span class="pre">Sequence</span><span class="p"><span class="pre">[</span></span><span class="pre">Sequence</span><span class="p"><span class="pre">[</span></span><span class="pre">FileAndLabel</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">other_streams_lists_of_files</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span> </span><span class="pre">Sequence</span><span class="p"><span class="pre">[</span></span><span class="pre">Sequence</span><span class="p"><span class="pre">[</span></span><span class="pre">FileAndLabel</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">task_labels</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Sequence</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">complete_test_set_only</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">bool</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">train_transform</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">train_target_transform</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">eval_transform</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">eval_target_transform</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">other_streams_transforms</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span> </span><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">Any</span><span class="p"><span class="pre">,</span> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dataset_type</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">AvalancheDatasetType</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">AvalancheDatasetType.UNDEFINED</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">GenericCLScenario</span></span></span><a class="reference internal" href="../../../../../_modules/avalanche/benchmarks/scenarios/generic_benchmark_creation/#create_generic_benchmark_from_paths"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.benchmarks.scenarios.generic_benchmark_creation.create_generic_benchmark_from_paths" title="Permalink to this definition"></a></dt>
<dd><p>Creates a benchmark instance given a sequence of lists of files. A separate
dataset will be created for each list. Each of those datasets
will be considered a separate experience.</p>
<p>This is very similar to <a class="reference internal" href="#avalanche.benchmarks.scenarios.generic_benchmark_creation.create_generic_benchmark_from_filelists" title="avalanche.benchmarks.scenarios.generic_benchmark_creation.create_generic_benchmark_from_filelists"><code class="xref py py-func docutils literal notranslate"><span class="pre">create_generic_benchmark_from_filelists()</span></code></a>,
with the main difference being that
<a class="reference internal" href="#avalanche.benchmarks.scenarios.generic_benchmark_creation.create_generic_benchmark_from_filelists" title="avalanche.benchmarks.scenarios.generic_benchmark_creation.create_generic_benchmark_from_filelists"><code class="xref py py-func docutils literal notranslate"><span class="pre">create_generic_benchmark_from_filelists()</span></code></a> accepts, for each
experience, a file list formatted in Caffe-style. On the contrary, this
accepts a list of tuples where each tuple contains two elements: the full
path to the pattern and its label. Optionally, the tuple may contain a third
element describing the bounding box of the element to crop. This last
bounding box may be useful when trying to extract the part of the image
depicting the desired element.</p>
<p>Apart from that, the same limitations of
<a class="reference internal" href="#avalanche.benchmarks.scenarios.generic_benchmark_creation.create_generic_benchmark_from_filelists" title="avalanche.benchmarks.scenarios.generic_benchmark_creation.create_generic_benchmark_from_filelists"><code class="xref py py-func docutils literal notranslate"><span class="pre">create_generic_benchmark_from_filelists()</span></code></a> regarding task labels apply.</p>
<p>The label of each pattern doesn’t have to be an int. Also, a dataset type
can be defined.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>train_lists_of_files</strong> – A list of lists. Each list describes the paths
and labels of patterns to include in that training experience, as
tuples. Each tuple must contain two elements: the full path to the
pattern and its class label. Optionally, the tuple may contain a
third element describing the bounding box to use for cropping (top,
left, height, width).</p></li>
<li><p><strong>test_lists_of_files</strong> – A list of lists. Each list describes the paths
and labels of patterns to include in that test experience, as tuples.
Each tuple must contain two elements: the full path to the pattern
and its class label. Optionally, the tuple may contain a third element
describing the bounding box to use for cropping (top, left, height,
width).</p></li>
<li><p><strong>other_streams_lists_of_files</strong> – A dictionary describing the content of
custom streams. Keys must be valid stream names (letters and numbers,
not starting with a number) while the value follow the same structure
of <cite>train_lists_of_files</cite> and <cite>test_lists_of_files</cite> parameters. If this
dictionary contains the definition for “train” or “test” streams then
those definition will  override the <cite>train_lists_of_files</cite> and
<cite>test_lists_of_files</cite> parameters.</p></li>
<li><p><strong>task_labels</strong> – A list of task labels. Must contain at least a value
for each experience. Each value describes the task label that will be
applied to all patterns of a certain experience. For more info on that,
see the function description.</p></li>
<li><p><strong>complete_test_set_only</strong> – If True, only the complete test set will
be returned by the benchmark. This means that the <code class="docutils literal notranslate"><span class="pre">test_list_of_files</span></code>
parameter must define a single experience (the complete test set).
Defaults to False.</p></li>
<li><p><strong>train_transform</strong> – The transformation to apply to the training data,
e.g. a random crop, a normalization or a concatenation of different
transformations (see torchvision.transform documentation for a
comprehensive list of possible transformations). Defaults to None.</p></li>
<li><p><strong>train_target_transform</strong> – The transformation to apply to training
patterns targets. Defaults to None.</p></li>
<li><p><strong>eval_transform</strong> – The transformation to apply to the test data,
e.g. a random crop, a normalization or a concatenation of different
transformations (see torchvision.transform documentation for a
comprehensive list of possible transformations). Defaults to None.</p></li>
<li><p><strong>eval_target_transform</strong> – The transformation to apply to test
patterns targets. Defaults to None.</p></li>
<li><p><strong>other_streams_transforms</strong> – Transformations to apply to custom
streams. If no transformations are defined for a custom stream,
then “train” transformations will be used. This parameter must be a
dictionary mapping stream names to transformations. The transformations
must be a two elements tuple where the first element defines the
X transformation while the second element is the Y transformation.
Those elements can be None. If this dictionary contains the
transformations for “train” or “test” streams then those transformations
will override the <cite>train_transform</cite>, <cite>train_target_transform</cite>,
<cite>eval_transform</cite> and <cite>eval_target_transform</cite> parameters.</p></li>
<li><p><strong>dataset_type</strong> – The type of the dataset. Defaults to UNDEFINED.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A <code class="xref py py-class docutils literal notranslate"><span class="pre">GenericCLScenario</span></code> instance.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="avalanche.benchmarks.scenarios.generic_benchmark_creation.create_generic_benchmark_from_tensor_lists">
<span class="sig-prename descclassname"><span class="pre">avalanche.benchmarks.scenarios.generic_benchmark_creation.</span></span><span class="sig-name descname"><span class="pre">create_generic_benchmark_from_tensor_lists</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">train_tensors</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Sequence</span><span class="p"><span class="pre">[</span></span><span class="pre">Sequence</span><span class="p"><span class="pre">[</span></span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">test_tensors</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Sequence</span><span class="p"><span class="pre">[</span></span><span class="pre">Sequence</span><span class="p"><span class="pre">[</span></span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">other_streams_tensors</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span> </span><span class="pre">Sequence</span><span class="p"><span class="pre">[</span></span><span class="pre">Sequence</span><span class="p"><span class="pre">[</span></span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">task_labels</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Sequence</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">complete_test_set_only</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">bool</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">train_transform</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">train_target_transform</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">eval_transform</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">eval_target_transform</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">other_streams_transforms</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span> </span><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">Any</span><span class="p"><span class="pre">,</span> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dataset_type</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">AvalancheDatasetType</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">GenericCLScenario</span></span></span><a class="reference internal" href="../../../../../_modules/avalanche/benchmarks/scenarios/generic_benchmark_creation/#create_generic_benchmark_from_tensor_lists"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.benchmarks.scenarios.generic_benchmark_creation.create_generic_benchmark_from_tensor_lists" title="Permalink to this definition"></a></dt>
<dd><p>Creates a benchmark instance given lists of Tensors. A separate dataset will
be created from each Tensor tuple (x, y, z, …) and each of those training
datasets will be considered a separate training experience. Using this
helper function is the lowest-level way to create a Continual Learning
benchmark. When possible, consider using higher level helpers.</p>
<p>Experiences are defined by passing lists of tensors as the <cite>train_tensors</cite>,
<cite>test_tensors</cite> (and <cite>other_streams_tensors</cite>) parameters. Those parameters
must be lists containing lists of tensors, one list for each experience.
Each tensor defines the value of a feature (“x”, “y”, “z”, …) for all
patterns of that experience.</p>
<p>By default the second tensor of each experience will be used to fill the
<cite>targets</cite> value (label of each pattern).</p>
<p>Beware that task labels can only be defined by choosing a single task label
for each experience (the same task label is applied to all patterns of
experiences sharing the same position in different streams).</p>
<p>When in need to create a benchmark instance in which task labels are defined
in a more fine-grained way, then consider using
<a class="reference internal" href="#avalanche.benchmarks.scenarios.generic_benchmark_creation.create_multi_dataset_generic_benchmark" title="avalanche.benchmarks.scenarios.generic_benchmark_creation.create_multi_dataset_generic_benchmark"><code class="xref py py-func docutils literal notranslate"><span class="pre">create_multi_dataset_generic_benchmark()</span></code></a> by passing properly
initialized <code class="xref py py-class docutils literal notranslate"><span class="pre">AvalancheDataset</span></code> instances.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>train_tensors</strong> – A list of lists. The first list must contain the
tensors for the first training experience (one tensor per feature), the
second list must contain the tensors for the second training experience,
and so on.</p></li>
<li><p><strong>test_tensors</strong> – A list of lists. The first list must contain the
tensors for the first test experience (one tensor per feature), the
second list must contain the tensors for the second test experience,
and so on. When using <cite>complete_test_set_only</cite>, this parameter
must be a list containing a single sub-list for the single test
experience.</p></li>
<li><p><strong>other_streams_tensors</strong> – A dictionary describing the content of
custom streams. Keys must be valid stream names (letters and numbers,
not starting with a number) while the value follow the same structure
of <cite>train_tensors</cite> and <cite>test_tensors</cite> parameters. If this
dictionary contains the definition for “train” or “test” streams then
those definition will  override the <cite>train_tensors</cite> and <cite>test_tensors</cite>
parameters.</p></li>
<li><p><strong>task_labels</strong> – A list of task labels. Must contain at least a value
for each experience. Each value describes the task label that will be
applied to all patterns of a certain experience. For more info on that,
see the function description.</p></li>
<li><p><strong>complete_test_set_only</strong> – If True, only the complete test set will
be returned by the benchmark. This means that <code class="docutils literal notranslate"><span class="pre">test_tensors</span></code> must
define a single experience. Defaults to False.</p></li>
<li><p><strong>train_transform</strong> – The transformation to apply to the training data,
e.g. a random crop, a normalization or a concatenation of different
transformations (see torchvision.transform documentation for a
comprehensive list of possible transformations). Defaults to None.</p></li>
<li><p><strong>train_target_transform</strong> – The transformation to apply to training
patterns targets. Defaults to None.</p></li>
<li><p><strong>eval_transform</strong> – The transformation to apply to the test data,
e.g. a random crop, a normalization or a concatenation of different
transformations (see torchvision.transform documentation for a
comprehensive list of possible transformations). Defaults to None.</p></li>
<li><p><strong>eval_target_transform</strong> – The transformation to apply to test
patterns targets. Defaults to None.</p></li>
<li><p><strong>other_streams_transforms</strong> – Transformations to apply to custom
streams. If no transformations are defined for a custom stream,
then “train” transformations will be used. This parameter must be a
dictionary mapping stream names to transformations. The transformations
must be a two elements tuple where the first element defines the
X transformation while the second element is the Y transformation.
Those elements can be None. If this dictionary contains the
transformations for “train” or “test” streams then those transformations
will override the <cite>train_transform</cite>, <cite>train_target_transform</cite>,
<cite>eval_transform</cite> and <cite>eval_target_transform</cite> parameters.</p></li>
<li><p><strong>dataset_type</strong> – The type of the dataset. Defaults to UNDEFINED.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A <code class="xref py py-class docutils literal notranslate"><span class="pre">GenericCLScenario</span></code> instance.</p>
</dd>
</dl>
</dd></dl>

</section>
</section>
</section>


           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2021, ContinualAI.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>