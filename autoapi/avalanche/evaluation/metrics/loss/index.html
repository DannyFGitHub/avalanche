<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>avalanche.evaluation.metrics.loss &mdash; Avalanche 0.1 documentation</title>
      <link rel="stylesheet" href="../../../../../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../../../../../_static/css/mystyle.css" type="text/css" />
      <link rel="stylesheet" href="../../../../../_static/graphviz.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../../../../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="../../../../../" id="documentation_options" src="../../../../../_static/documentation_options.js"></script>
        <script src="../../../../../_static/jquery.js"></script>
        <script src="../../../../../_static/underscore.js"></script>
        <script src="../../../../../_static/doctools.js"></script>
    <script src="../../../../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../../../../genindex/" />
    <link rel="search" title="Search" href="../../../../../search/" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../../../../../">
            <img src="../../../../../_static/avalanche_logo.png" class="logo" alt="Logo"/>
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../../../search/" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Avalanche API:</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="reference internal" href="../../../../">API Reference</a><ul class="current">
<li class="toctree-l2 current"><a class="reference internal" href="../../../"><code class="xref py py-mod docutils literal notranslate"><span class="pre">avalanche</span></code></a><ul class="current">
<li class="toctree-l3 current"><a class="reference internal" href="../../../#subpackages">Subpackages</a><ul class="current">
<li class="toctree-l4"><a class="reference internal" href="../../../benchmarks/"><code class="xref py py-mod docutils literal notranslate"><span class="pre">avalanche.benchmarks</span></code></a></li>
<li class="toctree-l4 current"><a class="reference internal" href="../../"><code class="xref py py-mod docutils literal notranslate"><span class="pre">avalanche.evaluation</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../logging/"><code class="xref py py-mod docutils literal notranslate"><span class="pre">avalanche.logging</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../models/"><code class="xref py py-mod docutils literal notranslate"><span class="pre">avalanche.models</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../training/"><code class="xref py py-mod docutils literal notranslate"><span class="pre">avalanche.training</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../#submodules">Submodules</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../core/"><code class="xref py py-mod docutils literal notranslate"><span class="pre">avalanche.core</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../#package-contents">Package Contents</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../#avalanche.__version__">__version__</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../../../">Avalanche</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../../../../" class="icon icon-home"></a> &raquo;</li>
      <li><code class="xref py py-mod docutils literal notranslate"><span class="pre">avalanche.evaluation.metrics.loss</span></code></li>
      <li class="wy-breadcrumbs-aside">
            <a href="../../../../../_sources/autoapi/avalanche/evaluation/metrics/loss/index.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="module-avalanche.evaluation.metrics.loss">
<span id="avalanche-evaluation-metrics-loss"></span><h1><a class="reference internal" href="#module-avalanche.evaluation.metrics.loss" title="avalanche.evaluation.metrics.loss"><code class="xref py py-mod docutils literal notranslate"><span class="pre">avalanche.evaluation.metrics.loss</span></code></a><a class="headerlink" href="#module-avalanche.evaluation.metrics.loss" title="Permalink to this headline"></a></h1>
<section id="module-contents">
<h2>Module Contents<a class="headerlink" href="#module-contents" title="Permalink to this headline"></a></h2>
<section id="classes">
<h3>Classes<a class="headerlink" href="#classes" title="Permalink to this headline"></a></h3>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#avalanche.evaluation.metrics.loss.Loss" title="avalanche.evaluation.metrics.loss.Loss"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Loss</span></code></a></p></td>
<td><p>The standalone Loss metric. This is a general metric</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#avalanche.evaluation.metrics.loss.MinibatchLoss" title="avalanche.evaluation.metrics.loss.MinibatchLoss"><code class="xref py py-obj docutils literal notranslate"><span class="pre">MinibatchLoss</span></code></a></p></td>
<td><p>The minibatch loss metric.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#avalanche.evaluation.metrics.loss.EpochLoss" title="avalanche.evaluation.metrics.loss.EpochLoss"><code class="xref py py-obj docutils literal notranslate"><span class="pre">EpochLoss</span></code></a></p></td>
<td><p>The average loss over a single training epoch.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#avalanche.evaluation.metrics.loss.RunningEpochLoss" title="avalanche.evaluation.metrics.loss.RunningEpochLoss"><code class="xref py py-obj docutils literal notranslate"><span class="pre">RunningEpochLoss</span></code></a></p></td>
<td><p>The average loss across all minibatches up to the current</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#avalanche.evaluation.metrics.loss.ExperienceLoss" title="avalanche.evaluation.metrics.loss.ExperienceLoss"><code class="xref py py-obj docutils literal notranslate"><span class="pre">ExperienceLoss</span></code></a></p></td>
<td><p>At the end of each experience, this metric reports</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#avalanche.evaluation.metrics.loss.StreamLoss" title="avalanche.evaluation.metrics.loss.StreamLoss"><code class="xref py py-obj docutils literal notranslate"><span class="pre">StreamLoss</span></code></a></p></td>
<td><p>At the end of the entire stream of experiences, this metric reports the</p></td>
</tr>
</tbody>
</table>
</section>
<section id="functions">
<h3>Functions<a class="headerlink" href="#functions" title="Permalink to this headline"></a></h3>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#avalanche.evaluation.metrics.loss.loss_metrics" title="avalanche.evaluation.metrics.loss.loss_metrics"><code class="xref py py-obj docutils literal notranslate"><span class="pre">loss_metrics</span></code></a>(*, minibatch=False, epoch=False, epoch_running=False, experience=False, stream=False) → List[PluginMetric]</p></td>
<td><p>Helper method that can be used to obtain the desired set of</p></td>
</tr>
</tbody>
</table>
<dl class="py class">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.loss.Loss">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">avalanche.evaluation.metrics.loss.</span></span><span class="sig-name descname"><span class="pre">Loss</span></span><a class="reference internal" href="../../../../../_modules/avalanche/evaluation/metrics/loss/#Loss"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.loss.Loss" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Metric[float]</span></code></p>
<p>The standalone Loss metric. This is a general metric
used to compute more specific ones.</p>
<p>Instances of this metric keeps the running average loss
over multiple &lt;prediction, target&gt; pairs of Tensors,
provided incrementally.
The “prediction” and “target” tensors may contain plain labels or
one-hot/logit vectors.</p>
<p>Each time <cite>result</cite> is called, this metric emits the average loss
across all predictions made since the last <cite>reset</cite>.</p>
<p>The reset method will bring the metric to its initial state. By default
this metric in its initial state will return a loss value of 0.</p>
<p>Creates an instance of the loss metric.</p>
<p>By default this metric in its initial state will return a loss
value of 0. The metric can be updated by using the <cite>update</cite> method
while the running loss can be retrieved using the <cite>result</cite> method.</p>
<dl class="py method">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.loss.Loss.update">
<span class="sig-name descname"><span class="pre">update</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">loss</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">patterns</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">task_label</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="../../../../../_modules/avalanche/evaluation/metrics/loss/#Loss.update"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.loss.Loss.update" title="Permalink to this definition"></a></dt>
<dd><p>Update the running loss given the loss Tensor and the minibatch size.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>loss</strong> – The loss Tensor. Different reduction types don’t affect
the result.</p></li>
<li><p><strong>patterns</strong> – The number of patterns in the minibatch.</p></li>
<li><p><strong>task_label</strong> – the task label associated to the current experience</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>None.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.loss.Loss.result">
<span class="sig-name descname"><span class="pre">result</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">task_label</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span> </span><span class="pre">float</span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="../../../../../_modules/avalanche/evaluation/metrics/loss/#Loss.result"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.loss.Loss.result" title="Permalink to this definition"></a></dt>
<dd><p>Retrieves the running average loss per pattern.</p>
<p>Calling this method will not change the internal state of the metric.
:param task_label: None to return metric values for all the task labels.</p>
<blockquote>
<div><p>If an int, return value only for that task label</p>
</div></blockquote>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>The running loss, as a float.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.loss.Loss.reset">
<span class="sig-name descname"><span class="pre">reset</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">task_label</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="../../../../../_modules/avalanche/evaluation/metrics/loss/#Loss.reset"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.loss.Loss.reset" title="Permalink to this definition"></a></dt>
<dd><p>Resets the metric.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>task_label</strong> – None to reset all metric values. If an int,
reset metric value corresponding to that task label.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>None.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.loss.MinibatchLoss">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">avalanche.evaluation.metrics.loss.</span></span><span class="sig-name descname"><span class="pre">MinibatchLoss</span></span><a class="reference internal" href="../../../../../_modules/avalanche/evaluation/metrics/loss/#MinibatchLoss"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.loss.MinibatchLoss" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">avalanche.evaluation.metrics.loss.LossPluginMetric</span></code></p>
<p>The minibatch loss metric.
This plugin metric only works at training time.</p>
<p>This metric computes the average loss over patterns
from a single minibatch.
It reports the result after each iteration.</p>
<p>If a more coarse-grained logging is needed, consider using
<a class="reference internal" href="#avalanche.evaluation.metrics.loss.EpochLoss" title="avalanche.evaluation.metrics.loss.EpochLoss"><code class="xref py py-class docutils literal notranslate"><span class="pre">EpochLoss</span></code></a> instead.</p>
<p>Creates an instance of the MinibatchLoss metric.</p>
<dl class="py method">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.loss.MinibatchLoss.__str__">
<span class="sig-name descname"><span class="pre">__str__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../../../_modules/avalanche/evaluation/metrics/loss/#MinibatchLoss.__str__"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.loss.MinibatchLoss.__str__" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.loss.EpochLoss">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">avalanche.evaluation.metrics.loss.</span></span><span class="sig-name descname"><span class="pre">EpochLoss</span></span><a class="reference internal" href="../../../../../_modules/avalanche/evaluation/metrics/loss/#EpochLoss"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.loss.EpochLoss" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">avalanche.evaluation.metrics.loss.LossPluginMetric</span></code></p>
<p>The average loss over a single training epoch.
This plugin metric only works at training time.</p>
<p>The loss will be logged after each training epoch by computing
the loss on the predicted patterns during the epoch divided by
the overall number of patterns encountered in that epoch.</p>
<p>Creates an instance of the EpochLoss metric.</p>
<dl class="py method">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.loss.EpochLoss.__str__">
<span class="sig-name descname"><span class="pre">__str__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../../../_modules/avalanche/evaluation/metrics/loss/#EpochLoss.__str__"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.loss.EpochLoss.__str__" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.loss.RunningEpochLoss">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">avalanche.evaluation.metrics.loss.</span></span><span class="sig-name descname"><span class="pre">RunningEpochLoss</span></span><a class="reference internal" href="../../../../../_modules/avalanche/evaluation/metrics/loss/#RunningEpochLoss"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.loss.RunningEpochLoss" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">avalanche.evaluation.metrics.loss.LossPluginMetric</span></code></p>
<p>The average loss across all minibatches up to the current
epoch iteration.
This plugin metric only works at training time.</p>
<p>At each iteration, this metric logs the loss averaged over all patterns
seen so far in the current epoch.
The metric resets its state after each training epoch.</p>
<p>Creates an instance of the RunningEpochLoss metric.</p>
<dl class="py method">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.loss.RunningEpochLoss.__str__">
<span class="sig-name descname"><span class="pre">__str__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../../../_modules/avalanche/evaluation/metrics/loss/#RunningEpochLoss.__str__"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.loss.RunningEpochLoss.__str__" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.loss.ExperienceLoss">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">avalanche.evaluation.metrics.loss.</span></span><span class="sig-name descname"><span class="pre">ExperienceLoss</span></span><a class="reference internal" href="../../../../../_modules/avalanche/evaluation/metrics/loss/#ExperienceLoss"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.loss.ExperienceLoss" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">avalanche.evaluation.metrics.loss.LossPluginMetric</span></code></p>
<p>At the end of each experience, this metric reports
the average loss over all patterns seen in that experience.
This plugin metric only works at eval time.</p>
<p>Creates an instance of ExperienceLoss metric</p>
<dl class="py method">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.loss.ExperienceLoss.__str__">
<span class="sig-name descname"><span class="pre">__str__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../../../_modules/avalanche/evaluation/metrics/loss/#ExperienceLoss.__str__"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.loss.ExperienceLoss.__str__" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.loss.StreamLoss">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">avalanche.evaluation.metrics.loss.</span></span><span class="sig-name descname"><span class="pre">StreamLoss</span></span><a class="reference internal" href="../../../../../_modules/avalanche/evaluation/metrics/loss/#StreamLoss"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.loss.StreamLoss" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">avalanche.evaluation.metrics.loss.LossPluginMetric</span></code></p>
<p>At the end of the entire stream of experiences, this metric reports the
average loss over all patterns seen in all experiences.
This plugin metric only works at eval time.</p>
<p>Creates an instance of StreamLoss metric</p>
<dl class="py method">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.loss.StreamLoss.__str__">
<span class="sig-name descname"><span class="pre">__str__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../../../_modules/avalanche/evaluation/metrics/loss/#StreamLoss.__str__"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.loss.StreamLoss.__str__" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="avalanche.evaluation.metrics.loss.loss_metrics">
<span class="sig-prename descclassname"><span class="pre">avalanche.evaluation.metrics.loss.</span></span><span class="sig-name descname"><span class="pre">loss_metrics</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">minibatch</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">epoch</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">epoch_running</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">experience</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">stream</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">PluginMetric</span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="../../../../../_modules/avalanche/evaluation/metrics/loss/#loss_metrics"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.evaluation.metrics.loss.loss_metrics" title="Permalink to this definition"></a></dt>
<dd><p>Helper method that can be used to obtain the desired set of
plugin metrics.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>minibatch</strong> – If True, will return a metric able to log
the minibatch loss at training time.</p></li>
<li><p><strong>epoch</strong> – If True, will return a metric able to log
the epoch loss at training time.</p></li>
<li><p><strong>epoch_running</strong> – If True, will return a metric able to log
the running epoch loss at training time.</p></li>
<li><p><strong>experience</strong> – If True, will return a metric able to log
the loss on each evaluation experience.</p></li>
<li><p><strong>stream</strong> – If True, will return a metric able to log
the loss averaged over the entire evaluation stream of experiences.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A list of plugin metrics.</p>
</dd>
</dl>
</dd></dl>

</section>
</section>
</section>


           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2021, ContinualAI.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>